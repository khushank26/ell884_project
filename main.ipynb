{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11600524,"sourceType":"datasetVersion","datasetId":7275344},{"sourceId":11748601,"sourceType":"datasetVersion","datasetId":7375467},{"sourceId":11751310,"sourceType":"datasetVersion","datasetId":7377402}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:57:13.222132Z","iopub.execute_input":"2025-05-10T04:57:13.222721Z","iopub.status.idle":"2025-05-10T04:57:13.864345Z","shell.execute_reply.started":"2025-05-10T04:57:13.222700Z","shell.execute_reply":"2025-05-10T04:57:13.863597Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"/kaggle/input/databased/database/solvency_ii/solvency_ii.sqlite\n/kaggle/input/databased/database/solvency_ii/schema.sql\n/kaggle/input/databased/database/voter_1/voter_1.sqlite\n/kaggle/input/databased/database/concert_singer/concert_singer.sqlite\n/kaggle/input/databased/database/concert_singer/schema.sql\n/kaggle/input/databased/database/apartment_rentals/apartment_rentals.sqlite\n/kaggle/input/databased/database/apartment_rentals/schema.sql\n/kaggle/input/databased/database/flight_1/flight_1.sqlite\n/kaggle/input/databased/database/flight_1/schema.sql\n/kaggle/input/databased/database/gymnast/gymnast.sqlite\n/kaggle/input/databased/database/gymnast/schema.sql\n/kaggle/input/databased/database/epinions_1/epinions_1.sqlite\n/kaggle/input/databased/database/local_govt_and_lot/local_govt_and_lot.sqlite\n/kaggle/input/databased/database/local_govt_and_lot/schema.sql\n/kaggle/input/databased/database/assets_maintenance/assets_maintenance.sqlite\n/kaggle/input/databased/database/assets_maintenance/schema.sql\n/kaggle/input/databased/database/customers_and_products_contacts/customers_and_products_contacts.sqlite\n/kaggle/input/databased/database/customers_and_products_contacts/schema.sql\n/kaggle/input/databased/database/restaurants/schema.sql\n/kaggle/input/databased/database/restaurants/restaurants.sqlite\n/kaggle/input/databased/database/insurance_policies/insurance_policies.sqlite\n/kaggle/input/databased/database/insurance_policies/schema.sql\n/kaggle/input/databased/database/local_govt_mdm/local_govt_mdm.sqlite\n/kaggle/input/databased/database/local_govt_mdm/schema.sql\n/kaggle/input/databased/database/county_public_safety/county_public_safety.sqlite\n/kaggle/input/databased/database/county_public_safety/schema.sql\n/kaggle/input/databased/database/restaurant_1/schema.sql\n/kaggle/input/databased/database/restaurant_1/restaurant_1.sqlite\n/kaggle/input/databased/database/ship_mission/ship_mission.sqlite\n/kaggle/input/databased/database/ship_mission/schema.sql\n/kaggle/input/databased/database/culture_company/culture_company.sqlite\n/kaggle/input/databased/database/culture_company/schema.sql\n/kaggle/input/databased/database/city_record/city_record.sqlite\n/kaggle/input/databased/database/city_record/schema.sql\n/kaggle/input/databased/database/inn_1/q.txt\n/kaggle/input/databased/database/inn_1/link.txt\n/kaggle/input/databased/database/inn_1/annotation.json\n/kaggle/input/databased/database/inn_1/inn_1.sql\n/kaggle/input/databased/database/inn_1/change_date.py\n/kaggle/input/databased/database/inn_1/inn_1.sqlite\n/kaggle/input/databased/database/inn_1/data_csv/Rooms.csv\n/kaggle/input/databased/database/inn_1/data_csv/README.INN.TXT\n/kaggle/input/databased/database/inn_1/data_csv/Reservations.csv\n/kaggle/input/databased/database/inn_1/data_csv/Reservations_t.csv\n/kaggle/input/databased/database/yelp/schema.sql\n/kaggle/input/databased/database/yelp/yelp.sqlite\n/kaggle/input/databased/database/insurance_and_eClaims/schema.sql\n/kaggle/input/databased/database/insurance_and_eClaims/insurance_and_eClaims.sqlite\n/kaggle/input/databased/database/tracking_grants_for_research/schema.sql\n/kaggle/input/databased/database/tracking_grants_for_research/tracking_grants_for_research.sqlite\n/kaggle/input/databased/database/machine_repair/machine_repair.sqlite\n/kaggle/input/databased/database/machine_repair/schema.sql\n/kaggle/input/databased/database/mountain_photos/mountain_photos.sqlite\n/kaggle/input/databased/database/mountain_photos/schema.sql\n/kaggle/input/databased/database/storm_record/schema.sql\n/kaggle/input/databased/database/storm_record/storm_record.sqlite\n/kaggle/input/databased/database/wedding/wedding.sqlite\n/kaggle/input/databased/database/wedding/schema.sql\n/kaggle/input/databased/database/train_station/schema.sql\n/kaggle/input/databased/database/train_station/train_station.sqlite\n/kaggle/input/databased/database/party_host/party_host.sqlite\n/kaggle/input/databased/database/party_host/schema.sql\n/kaggle/input/databased/database/csu_1/schema.sql\n/kaggle/input/databased/database/csu_1/csu_1.sqlite\n/kaggle/input/databased/database/company_employee/company_employee.sqlite\n/kaggle/input/databased/database/company_employee/schema.sql\n/kaggle/input/databased/database/entertainment_awards/schema.sql\n/kaggle/input/databased/database/entertainment_awards/entertainment_awards.sqlite\n/kaggle/input/databased/database/manufacturer/manufacturer.sqlite\n/kaggle/input/databased/database/manufacturer/schema.sql\n/kaggle/input/databased/database/behavior_monitoring/behavior_monitoring.sqlite\n/kaggle/input/databased/database/behavior_monitoring/schema.sql\n/kaggle/input/databased/database/election_representative/election_representative.sqlite\n/kaggle/input/databased/database/election_representative/schema.sql\n/kaggle/input/databased/database/orchestra/orchestra.sqlite\n/kaggle/input/databased/database/orchestra/schema.sql\n/kaggle/input/databased/database/cre_Doc_Template_Mgt/schema.sql\n/kaggle/input/databased/database/cre_Doc_Template_Mgt/cre_Doc_Template_Mgt.sqlite\n/kaggle/input/databased/database/driving_school/schema.sql\n/kaggle/input/databased/database/driving_school/driving_school.sqlite\n/kaggle/input/databased/database/loan_1/loan_1.sqlite\n/kaggle/input/databased/database/loan_1/schema.sql\n/kaggle/input/databased/database/film_rank/film_rank.sqlite\n/kaggle/input/databased/database/film_rank/schema.sql\n/kaggle/input/databased/database/pilot_record/pilot_record.sqlite\n/kaggle/input/databased/database/pilot_record/schema.sql\n/kaggle/input/databased/database/network_2/schema.sql\n/kaggle/input/databased/database/network_2/network_2.sqlite\n/kaggle/input/databased/database/baseball_1/baseball_1.sqlite\n/kaggle/input/databased/database/baseball_1/schema.sql\n/kaggle/input/databased/database/candidate_poll/candidate_poll.sqlite\n/kaggle/input/databased/database/candidate_poll/schema.sql\n/kaggle/input/databased/database/scientist_1/schema.sql\n/kaggle/input/databased/database/scientist_1/scientist_1.sqlite\n/kaggle/input/databased/database/wrestler/schema.sql\n/kaggle/input/databased/database/wrestler/wrestler.sqlite\n/kaggle/input/databased/database/dog_kennels/schema.sql\n/kaggle/input/databased/database/dog_kennels/dog_kennels.sqlite\n/kaggle/input/databased/database/customers_campaigns_ecommerce/schema.sql\n/kaggle/input/databased/database/customers_campaigns_ecommerce/customers_campaigns_ecommerce.sqlite\n/kaggle/input/databased/database/performance_attendance/performance_attendance.sqlite\n/kaggle/input/databased/database/performance_attendance/schema.sql\n/kaggle/input/databased/database/device/device.sqlite\n/kaggle/input/databased/database/device/schema.sql\n/kaggle/input/databased/database/course_teach/course_teach.sqlite\n/kaggle/input/databased/database/course_teach/schema.sql\n/kaggle/input/databased/database/gas_company/gas_company.sqlite\n/kaggle/input/databased/database/gas_company/schema.sql\n/kaggle/input/databased/database/imdb/imdb.sqlite\n/kaggle/input/databased/database/imdb/schema.sql\n/kaggle/input/databased/database/twitter_1/twitter_1.sqlite\n/kaggle/input/databased/database/twitter_1/queries/postgres-dialects.xml\n/kaggle/input/databased/database/twitter_1/queries/oracle-dialects.xml\n/kaggle/input/databased/database/twitter_1/queries/sqlserver-dialects.xml\n/kaggle/input/databased/database/flight_company/flight_company.sqlite\n/kaggle/input/databased/database/flight_company/schema.sql\n/kaggle/input/databased/database/hr_1/schema.sql\n/kaggle/input/databased/database/hr_1/hr_1.sqlite\n/kaggle/input/databased/database/swimming/swimming.sqlite\n/kaggle/input/databased/database/swimming/schema.sql\n/kaggle/input/databased/database/architecture/architecture.sqlite\n/kaggle/input/databased/database/architecture/schema.sql\n/kaggle/input/databased/database/world_1/world_1.json\n/kaggle/input/databased/database/world_1/world_1.sqlite\n/kaggle/input/databased/database/match_season/schema.sql\n/kaggle/input/databased/database/match_season/match_season.sqlite\n/kaggle/input/databased/database/soccer_1/soccer_1.sqlite\n/kaggle/input/databased/database/soccer_1/schema.sql\n/kaggle/input/databased/database/formula_1/formula_1.splite\n/kaggle/input/databased/database/formula_1/annotation.json\n/kaggle/input/databased/database/formula_1/formula_1.sqlite\n/kaggle/input/databased/database/formula_1/formula_1.sql\n/kaggle/input/databased/database/formula_1/data_csv/races.csv\n/kaggle/input/databased/database/formula_1/data_csv/drivers.csv\n/kaggle/input/databased/database/formula_1/data_csv/constructors.csv\n/kaggle/input/databased/database/formula_1/data_csv/pitStops.csv\n/kaggle/input/databased/database/formula_1/data_csv/status.csv\n/kaggle/input/databased/database/formula_1/data_csv/seasons.csv\n/kaggle/input/databased/database/formula_1/data_csv/constructorStandings.csv\n/kaggle/input/databased/database/formula_1/data_csv/driverStandings.csv\n/kaggle/input/databased/database/formula_1/data_csv/constructorResults.csv\n/kaggle/input/databased/database/formula_1/data_csv/lapTimes.csv\n/kaggle/input/databased/database/formula_1/data_csv/results.csv\n/kaggle/input/databased/database/formula_1/data_csv/circuits.csv\n/kaggle/input/databased/database/formula_1/data_csv/qualifying.csv\n/kaggle/input/databased/database/car_1/car_1.json\n/kaggle/input/databased/database/car_1/q.txt\n/kaggle/input/databased/database/car_1/car_1.sqlite\n/kaggle/input/databased/database/car_1/link.txt\n/kaggle/input/databased/database/car_1/annotation.json\n/kaggle/input/databased/database/car_1/car_1.sql\n/kaggle/input/databased/database/car_1/data_csv/countries.csv\n/kaggle/input/databased/database/car_1/data_csv/cars-data.csv\n/kaggle/input/databased/database/car_1/data_csv/model-list.csv\n/kaggle/input/databased/database/car_1/data_csv/continents.csv\n/kaggle/input/databased/database/car_1/data_csv/car-makers.csv\n/kaggle/input/databased/database/car_1/data_csv/car-names.csv\n/kaggle/input/databased/database/car_1/data_csv/cars.desc\n/kaggle/input/databased/database/car_1/data_csv/README.CARS.TXT\n/kaggle/input/databased/database/university_basketball/schema.sql\n/kaggle/input/databased/database/university_basketball/university_basketball.sqlite\n/kaggle/input/databased/database/body_builder/body_builder.sqlite\n/kaggle/input/databased/database/body_builder/schema.sql\n/kaggle/input/databased/database/railway/schema.sql\n/kaggle/input/databased/database/railway/railway.sqlite\n/kaggle/input/databased/database/medicine_enzyme_interaction/medicine_enzyme_interaction.sqlite\n/kaggle/input/databased/database/medicine_enzyme_interaction/schema.sql\n/kaggle/input/databased/database/tracking_share_transactions/tracking_share_transactions.sqlite\n/kaggle/input/databased/database/tracking_share_transactions/schema.sql\n/kaggle/input/databased/database/musical/schema.sql\n/kaggle/input/databased/database/musical/musical.sqlite\n/kaggle/input/databased/database/tracking_software_problems/tracking_software_problems.sqlite\n/kaggle/input/databased/database/tracking_software_problems/schema.sql\n/kaggle/input/databased/database/academic/academic.sqlite\n/kaggle/input/databased/database/academic/schema.sql\n/kaggle/input/databased/database/products_for_hire/schema.sql\n/kaggle/input/databased/database/products_for_hire/products_for_hire.sqlite\n/kaggle/input/databased/database/roller_coaster/roller_coaster.sqlite\n/kaggle/input/databased/database/roller_coaster/schema.sql\n/kaggle/input/databased/database/tvshow/tvshow.sqlite\n/kaggle/input/databased/database/tvshow/schema.sql\n/kaggle/input/databased/database/college_3/schema.sql\n/kaggle/input/databased/database/college_3/college_3.sqlite\n/kaggle/input/databased/database/cinema/cinema.sqlite\n/kaggle/input/databased/database/cinema/schema.sql\n/kaggle/input/databased/database/college_2/link.txt\n/kaggle/input/databased/database/college_2/college_2.sqlite\n/kaggle/input/databased/database/college_2/TextBookExampleSchema.sql\n/kaggle/input/databased/database/tracking_orders/schema.sql\n/kaggle/input/databased/database/tracking_orders/tracking_orders.sqlite\n/kaggle/input/databased/database/school_bus/school_bus.sqlite\n/kaggle/input/databased/database/school_bus/schema.sql\n/kaggle/input/databased/database/riding_club/riding_club.sqlite\n/kaggle/input/databased/database/riding_club/schema.sql\n/kaggle/input/databased/database/geo/geo.sqlite\n/kaggle/input/databased/database/geo/schema.sql\n/kaggle/input/databased/database/bike_1/bike_1.sqlite\n/kaggle/input/databased/database/bike_1/schema.sql\n/kaggle/input/databased/database/phone_1/phone_1.sqlite\n/kaggle/input/databased/database/phone_1/schema.sql\n/kaggle/input/databased/database/wine_1/q.txt\n/kaggle/input/databased/database/wine_1/link.txt\n/kaggle/input/databased/database/wine_1/wine_1.sqlite\n/kaggle/input/databased/database/wine_1/wine_1.sql\n/kaggle/input/databased/database/wine_1/annotation.json\n/kaggle/input/databased/database/wine_1/data_csv/grapes.csv\n/kaggle/input/databased/database/wine_1/data_csv/README.WINE.txt\n/kaggle/input/databased/database/wine_1/data_csv/appellations.csv\n/kaggle/input/databased/database/wine_1/data_csv/wine.csv\n/kaggle/input/databased/database/flight_2/flight_2.json\n/kaggle/input/databased/database/flight_2/q.txt\n/kaggle/input/databased/database/flight_2/flight_2.sqlite\n/kaggle/input/databased/database/flight_2/link.txt\n/kaggle/input/databased/database/flight_2/annotation.json\n/kaggle/input/databased/database/flight_2/flight_2.sql\n/kaggle/input/databased/database/flight_2/data_csv/airports100.csv\n/kaggle/input/databased/database/flight_2/data_csv/airlines.csv\n/kaggle/input/databased/database/flight_2/data_csv/README.AIRLINES.txt\n/kaggle/input/databased/database/flight_2/data_csv/flights.csv\n/kaggle/input/databased/database/soccer_2/schema.sql\n/kaggle/input/databased/database/soccer_2/soccer_2.sqlite\n/kaggle/input/databased/database/customers_and_addresses/schema.sql\n/kaggle/input/databased/database/customers_and_addresses/customers_and_addresses.sqlite\n/kaggle/input/databased/database/chinook_1/chinook_1.sqlite\n/kaggle/input/databased/database/chinook_1/annotation.json\n/kaggle/input/databased/database/station_weather/station_weather.sqlite\n/kaggle/input/databased/database/station_weather/schema.sql\n/kaggle/input/databased/database/insurance_fnol/insurance_fnol.sqlite\n/kaggle/input/databased/database/insurance_fnol/schema.sql\n/kaggle/input/databased/database/student_1/q.txt\n/kaggle/input/databased/database/student_1/link.txt\n/kaggle/input/databased/database/student_1/annotation.json\n/kaggle/input/databased/database/student_1/student_1.sqlite\n/kaggle/input/databased/database/student_1/student_1.sql\n/kaggle/input/databased/database/student_1/data_csv/list.csv\n/kaggle/input/databased/database/student_1/data_csv/teachers.csv\n/kaggle/input/databased/database/student_1/data_csv/README.STUDENTS.TXT\n/kaggle/input/databased/database/cre_Doc_Control_Systems/cre_Doc_Control_Systems.sqlite\n/kaggle/input/databased/database/cre_Doc_Control_Systems/schema.sql\n/kaggle/input/databased/database/school_finance/school_finance.sqlite\n/kaggle/input/databased/database/school_finance/schema.sql\n/kaggle/input/databased/database/singer/singer.sqlite\n/kaggle/input/databased/database/singer/schema.sql\n/kaggle/input/databased/database/entrepreneur/entrepreneur.sqlite\n/kaggle/input/databased/database/entrepreneur/schema.sql\n/kaggle/input/databased/database/game_1/schema.sql\n/kaggle/input/databased/database/game_1/game_1.sqlite\n/kaggle/input/databased/database/scholar/schema.sql\n/kaggle/input/databased/database/scholar/scholar.sqlite\n/kaggle/input/databased/database/customer_complaints/schema.sql\n/kaggle/input/databased/database/customer_complaints/customer_complaints.sqlite\n/kaggle/input/databased/database/sakila_1/sakila_1.sqlite\n/kaggle/input/databased/database/sakila_1/schema.sql\n/kaggle/input/databased/database/voter_2/schema.sql\n/kaggle/input/databased/database/voter_2/voter_2.sqlite\n/kaggle/input/databased/database/pets_1/schema.sql\n/kaggle/input/databased/database/pets_1/pets_1.sqlite\n/kaggle/input/databased/database/music_1/music_1.sqlite\n/kaggle/input/databased/database/music_1/schema.sql\n/kaggle/input/databased/database/browser_web/browser_web.sqlite\n/kaggle/input/databased/database/browser_web/schema.sql\n/kaggle/input/databased/database/wta_1/wta_1.sqlite\n/kaggle/input/databased/database/wta_1/wta_1.sql\n/kaggle/input/databased/database/cre_Doc_Tracking_DB/cre_Doc_Tracking_DB.sqlite\n/kaggle/input/databased/database/cre_Doc_Tracking_DB/schema.sql\n/kaggle/input/databased/database/cre_Theme_park/cre_Theme_park.sqlite\n/kaggle/input/databased/database/cre_Theme_park/schema.sql\n/kaggle/input/databased/database/e_learning/schema.sql\n/kaggle/input/databased/database/e_learning/e_learning.sqlite\n/kaggle/input/databased/database/school_player/school_player.sqlite\n/kaggle/input/databased/database/school_player/schema.sql\n/kaggle/input/databased/database/movie_1/movie_1.sqlite\n/kaggle/input/databased/database/movie_1/schema.sql\n/kaggle/input/databased/database/flight_4/flight_4.sqlite\n/kaggle/input/databased/database/flight_4/link.txt\n/kaggle/input/databased/database/flight_4/sql.txt\n/kaggle/input/databased/database/activity_1/activity_1.sqlite\n/kaggle/input/databased/database/activity_1/schema.sql\n/kaggle/input/databased/database/perpetrator/perpetrator.sqlite\n/kaggle/input/databased/database/perpetrator/schema.sql\n/kaggle/input/databased/database/race_track/race_track.sqlite\n/kaggle/input/databased/database/race_track/schema.sql\n/kaggle/input/databased/database/decoration_competition/decoration_competition.sqlite\n/kaggle/input/databased/database/decoration_competition/schema.sql\n/kaggle/input/databased/database/employee_hire_evaluation/schema.sql\n/kaggle/input/databased/database/employee_hire_evaluation/employee_hire_evaluation.sqlite\n/kaggle/input/databased/database/ship_1/ship_1.sqlite\n/kaggle/input/databased/database/ship_1/schema.sql\n/kaggle/input/databased/database/election/election.sqlite\n/kaggle/input/databased/database/election/schema.sql\n/kaggle/input/databased/database/aircraft/aircraft.sqlite\n/kaggle/input/databased/database/aircraft/schema.sql\n/kaggle/input/databased/database/theme_gallery/schema.sql\n/kaggle/input/databased/database/theme_gallery/theme_gallery.sqlite\n/kaggle/input/databased/database/department_management/department_management.sqlite\n/kaggle/input/databased/database/department_management/schema.sql\n/kaggle/input/databased/database/product_catalog/product_catalog.sqlite\n/kaggle/input/databased/database/product_catalog/schema.sql\n/kaggle/input/databased/database/department_store/department_store.sqlite\n/kaggle/input/databased/database/department_store/schema.sql\n/kaggle/input/databased/database/college_1/college_1.sqlite\n/kaggle/input/databased/database/college_1/TinyCollege.sql\n/kaggle/input/databased/database/college_1/link.txt\n/kaggle/input/databased/database/workshop_paper/workshop_paper.sqlite\n/kaggle/input/databased/database/workshop_paper/schema.sql\n/kaggle/input/databased/database/protein_institute/protein_institute.sqlite\n/kaggle/input/databased/database/protein_institute/schema.sql\n/kaggle/input/databased/database/network_1/network_1.sqlite\n/kaggle/input/databased/database/network_1/schema.sql\n/kaggle/input/databased/database/game_injury/game_injury.sqlite\n/kaggle/input/databased/database/game_injury/schema.sql\n/kaggle/input/databased/database/e_government/e_government.sqlite\n/kaggle/input/databased/database/e_government/schema.sql\n/kaggle/input/databased/database/dorm_1/schema.sql\n/kaggle/input/databased/database/dorm_1/dorm_1.sqlite\n/kaggle/input/databased/database/store_product/store_product.sqlite\n/kaggle/input/databased/database/store_product/schema.sql\n/kaggle/input/databased/database/cre_Docs_and_Epenses/cre_Docs_and_Epenses.sqlite\n/kaggle/input/databased/database/cre_Docs_and_Epenses/schema.sql\n/kaggle/input/databased/database/club_1/club_1.sqlite\n/kaggle/input/databased/database/club_1/schema.sql\n/kaggle/input/databased/database/small_bank_1/small_bank_1.sqlite\n/kaggle/input/databased/database/program_share/schema.sql\n/kaggle/input/databased/database/program_share/program_share.sqlite\n/kaggle/input/databased/database/cre_Drama_Workshop_Groups/cre_Drama_Workshop_Groups.sqlite\n/kaggle/input/databased/database/cre_Drama_Workshop_Groups/schema.sql\n/kaggle/input/databased/database/phone_market/phone_market.sqlite\n/kaggle/input/databased/database/phone_market/schema.sql\n/kaggle/input/databased/database/company_office/schema.sql\n/kaggle/input/databased/database/company_office/company_office.sqlite\n/kaggle/input/databased/database/local_govt_in_alabama/local_govt_in_alabama.sqlite\n/kaggle/input/databased/database/local_govt_in_alabama/schema.sql\n/kaggle/input/databased/database/store_1/schema.sql\n/kaggle/input/databased/database/store_1/store_1.sqlite\n/kaggle/input/databased/database/company_1/company_1.sqlite\n/kaggle/input/databased/database/company_1/link.txt\n/kaggle/input/databased/database/debate/schema.sql\n/kaggle/input/databased/database/debate/debate.sqlite\n/kaggle/input/databased/database/battle_death/battle_death.sqlite\n/kaggle/input/databased/database/battle_death/schema.sql\n/kaggle/input/databased/database/customers_card_transactions/schema.sql\n/kaggle/input/databased/database/customers_card_transactions/customers_card_transactions.sqlite\n/kaggle/input/databased/database/music_2/music_2.sqlite\n/kaggle/input/databased/database/music_2/schema.sql\n/kaggle/input/databased/database/customer_deliveries/customer_deliveries.sqlite\n/kaggle/input/databased/database/customer_deliveries/schema.sql\n/kaggle/input/databased/database/poker_player/schema.sql\n/kaggle/input/databased/database/poker_player/poker_player.sqlite\n/kaggle/input/databased/database/coffee_shop/schema.sql\n/kaggle/input/databased/database/coffee_shop/coffee_shop.sqlite\n/kaggle/input/databased/database/manufactory_1/schema.sql\n/kaggle/input/databased/database/manufactory_1/manufactory_1.sqlite\n/kaggle/input/databased/database/allergy_1/allergy_1.sqlite\n/kaggle/input/databased/database/allergy_1/schema.sql\n/kaggle/input/databased/database/music_4/schema.sql\n/kaggle/input/databased/database/music_4/music_4.sqlite\n/kaggle/input/databased/database/document_management/document_management.sqlite\n/kaggle/input/databased/database/document_management/schema.sql\n/kaggle/input/databased/database/hospital_1/schema.sql\n/kaggle/input/databased/database/hospital_1/hospital_1.sqlite\n/kaggle/input/databased/database/student_assessment/student_assessment.sqlite\n/kaggle/input/databased/database/student_assessment/schema.sql\n/kaggle/input/databased/database/sports_competition/schema.sql\n/kaggle/input/databased/database/sports_competition/sports_competition.sqlite\n/kaggle/input/databased/database/journal_committee/journal_committee.sqlite\n/kaggle/input/databased/database/journal_committee/schema.sql\n/kaggle/input/databased/database/farm/schema.sql\n/kaggle/input/databased/database/farm/farm.sqlite\n/kaggle/input/databased/database/book_2/book_2.sqlite\n/kaggle/input/databased/database/book_2/schema.sql\n/kaggle/input/databased/database/climbing/climbing.sqlite\n/kaggle/input/databased/database/climbing/schema.sql\n/kaggle/input/databased/database/real_estate_properties/schema.sql\n/kaggle/input/databased/database/real_estate_properties/real_estate_properties.sqlite\n/kaggle/input/databased/database/news_report/news_report.sqlite\n/kaggle/input/databased/database/news_report/schema.sql\n/kaggle/input/databased/database/customers_and_invoices/customers_and_invoices.sqlite\n/kaggle/input/databased/database/customers_and_invoices/schema.sql\n/kaggle/input/databased/database/shop_membership/schema.sql\n/kaggle/input/databased/database/shop_membership/shop_membership.sqlite\n/kaggle/input/databased/database/party_people/party_people.sqlite\n/kaggle/input/databased/database/party_people/schema.sql\n/kaggle/input/databased/database/museum_visit/schema.sql\n/kaggle/input/databased/database/museum_visit/museum_visit.sqlite\n/kaggle/input/databased/database/icfp_1/q.txt\n/kaggle/input/databased/database/icfp_1/link.txt\n/kaggle/input/databased/database/icfp_1/icfp_1.sqlite\n/kaggle/input/databased/database/student_transcripts_tracking/schema.sql\n/kaggle/input/databased/database/student_transcripts_tracking/student_transcripts_tracking.sqlite\n/kaggle/input/databased/database/products_gen_characteristics/products_gen_characteristics.sqlite\n/kaggle/input/databased/database/products_gen_characteristics/schema.sql\n/kaggle/input/databased/test_database/solvency_ii/solvency_ii.sqlite\n/kaggle/input/databased/test_database/solvency_ii/schema.sql\n/kaggle/input/databased/test_database/voter_1/voter_1.sqlite\n/kaggle/input/databased/test_database/concert_singer/concert_singer.sqlite\n/kaggle/input/databased/test_database/concert_singer/schema.sql\n/kaggle/input/databased/test_database/apartment_rentals/apartment_rentals.sqlite\n/kaggle/input/databased/test_database/apartment_rentals/schema.sql\n/kaggle/input/databased/test_database/flight_1/flight_1.sqlite\n/kaggle/input/databased/test_database/flight_1/schema.sql\n/kaggle/input/databased/test_database/country_language/schema.sql\n/kaggle/input/databased/test_database/country_language/country_language.sqlite\n/kaggle/input/databased/test_database/gymnast/gymnast.sqlite\n/kaggle/input/databased/test_database/gymnast/schema.sql\n/kaggle/input/databased/test_database/epinions_1/epinions_1.sqlite\n/kaggle/input/databased/test_database/government_shift/government_shift.sqlite\n/kaggle/input/databased/test_database/government_shift/schema.sql\n/kaggle/input/databased/test_database/customers_and_orders/schema.sql\n/kaggle/input/databased/test_database/customers_and_orders/customers_and_orders.sqlite\n/kaggle/input/databased/test_database/local_govt_and_lot/local_govt_and_lot.sqlite\n/kaggle/input/databased/test_database/local_govt_and_lot/schema.sql\n/kaggle/input/databased/test_database/assets_maintenance/assets_maintenance.sqlite\n/kaggle/input/databased/test_database/assets_maintenance/schema.sql\n/kaggle/input/databased/test_database/customers_and_products_contacts/customers_and_products_contacts.sqlite\n/kaggle/input/databased/test_database/customers_and_products_contacts/schema.sql\n/kaggle/input/databased/test_database/restaurants/schema.sql\n/kaggle/input/databased/test_database/restaurants/restaurants.sqlite\n/kaggle/input/databased/test_database/insurance_policies/insurance_policies.sqlite\n/kaggle/input/databased/test_database/insurance_policies/schema.sql\n/kaggle/input/databased/test_database/local_govt_mdm/local_govt_mdm.sqlite\n/kaggle/input/databased/test_database/local_govt_mdm/schema.sql\n/kaggle/input/databased/test_database/county_public_safety/county_public_safety.sqlite\n/kaggle/input/databased/test_database/county_public_safety/schema.sql\n/kaggle/input/databased/test_database/book_1/schema_old.sql\n/kaggle/input/databased/test_database/book_1/q.txt\n/kaggle/input/databased/test_database/book_1/link.txt\n/kaggle/input/databased/test_database/book_1/schema.sql\n/kaggle/input/databased/test_database/book_1/annotation.json\n/kaggle/input/databased/test_database/book_1/book_1.sqlite\n/kaggle/input/databased/test_database/book_1/sql.txt\n/kaggle/input/databased/test_database/restaurant_1/schema.sql\n/kaggle/input/databased/test_database/restaurant_1/restaurant_1.sqlite\n/kaggle/input/databased/test_database/ship_mission/ship_mission.sqlite\n/kaggle/input/databased/test_database/ship_mission/schema.sql\n/kaggle/input/databased/test_database/culture_company/culture_company.sqlite\n/kaggle/input/databased/test_database/culture_company/schema.sql\n/kaggle/input/databased/test_database/city_record/city_record.sqlite\n/kaggle/input/databased/test_database/city_record/schema.sql\n/kaggle/input/databased/test_database/inn_1/q.txt\n/kaggle/input/databased/test_database/inn_1/link.txt\n/kaggle/input/databased/test_database/inn_1/annotation.json\n/kaggle/input/databased/test_database/inn_1/inn_1.sql\n/kaggle/input/databased/test_database/inn_1/change_date.py\n/kaggle/input/databased/test_database/inn_1/inn_1.sqlite\n/kaggle/input/databased/test_database/inn_1/data_csv/Rooms.csv\n/kaggle/input/databased/test_database/inn_1/data_csv/README.INN.TXT\n/kaggle/input/databased/test_database/inn_1/data_csv/Reservations.csv\n/kaggle/input/databased/test_database/inn_1/data_csv/Reservations_t.csv\n/kaggle/input/databased/test_database/yelp/schema.sql\n/kaggle/input/databased/test_database/yelp/yelp.sqlite\n/kaggle/input/databased/test_database/insurance_and_eClaims/schema.sql\n/kaggle/input/databased/test_database/insurance_and_eClaims/insurance_and_eClaims.sqlite\n/kaggle/input/databased/test_database/tracking_grants_for_research/schema.sql\n/kaggle/input/databased/test_database/tracking_grants_for_research/tracking_grants_for_research.sqlite\n/kaggle/input/databased/test_database/machine_repair/machine_repair.sqlite\n/kaggle/input/databased/test_database/machine_repair/schema.sql\n/kaggle/input/databased/test_database/mountain_photos/mountain_photos.sqlite\n/kaggle/input/databased/test_database/mountain_photos/schema.sql\n/kaggle/input/databased/test_database/storm_record/schema.sql\n/kaggle/input/databased/test_database/storm_record/storm_record.sqlite\n/kaggle/input/databased/test_database/warehouse_1/q.txt\n/kaggle/input/databased/test_database/warehouse_1/link.txt\n/kaggle/input/databased/test_database/warehouse_1/schema.sql\n/kaggle/input/databased/test_database/warehouse_1/warehouse_1.sqlite\n/kaggle/input/databased/test_database/warehouse_1/annotation.json\n/kaggle/input/databased/test_database/warehouse_1/sql.txt\n/kaggle/input/databased/test_database/wedding/wedding.sqlite\n/kaggle/input/databased/test_database/wedding/schema.sql\n/kaggle/input/databased/test_database/train_station/schema.sql\n/kaggle/input/databased/test_database/train_station/train_station.sqlite\n/kaggle/input/databased/test_database/party_host/party_host.sqlite\n/kaggle/input/databased/test_database/party_host/schema.sql\n/kaggle/input/databased/test_database/csu_1/schema.sql\n/kaggle/input/databased/test_database/csu_1/csu_1.sqlite\n/kaggle/input/databased/test_database/company_employee/company_employee.sqlite\n/kaggle/input/databased/test_database/company_employee/schema.sql\n/kaggle/input/databased/test_database/entertainment_awards/schema.sql\n/kaggle/input/databased/test_database/entertainment_awards/entertainment_awards.sqlite\n/kaggle/input/databased/test_database/manufacturer/manufacturer.sqlite\n/kaggle/input/databased/test_database/manufacturer/schema.sql\n/kaggle/input/databased/test_database/behavior_monitoring/behavior_monitoring.sqlite\n/kaggle/input/databased/test_database/behavior_monitoring/schema.sql\n/kaggle/input/databased/test_database/election_representative/election_representative.sqlite\n/kaggle/input/databased/test_database/election_representative/schema.sql\n/kaggle/input/databased/test_database/orchestra/orchestra.sqlite\n/kaggle/input/databased/test_database/orchestra/schema.sql\n/kaggle/input/databased/test_database/planet_1/q.txt\n/kaggle/input/databased/test_database/planet_1/planet_1.sqlite\n/kaggle/input/databased/test_database/planet_1/link.txt\n/kaggle/input/databased/test_database/planet_1/schema.sql\n/kaggle/input/databased/test_database/planet_1/annotation.json\n/kaggle/input/databased/test_database/planet_1/sql.txt\n/kaggle/input/databased/test_database/cre_Doc_Template_Mgt/schema.sql\n/kaggle/input/databased/test_database/cre_Doc_Template_Mgt/cre_Doc_Template_Mgt.sqlite\n/kaggle/input/databased/test_database/driving_school/schema.sql\n/kaggle/input/databased/test_database/driving_school/driving_school.sqlite\n/kaggle/input/databased/test_database/loan_1/loan_1.sqlite\n/kaggle/input/databased/test_database/loan_1/schema.sql\n/kaggle/input/databased/test_database/film_rank/film_rank.sqlite\n/kaggle/input/databased/test_database/film_rank/schema.sql\n/kaggle/input/databased/test_database/pilot_record/pilot_record.sqlite\n/kaggle/input/databased/test_database/pilot_record/schema.sql\n/kaggle/input/databased/test_database/pilot_1/pilot_1.sqlite\n/kaggle/input/databased/test_database/pilot_1/link.txt\n/kaggle/input/databased/test_database/pilot_1/schema.sql\n/kaggle/input/databased/test_database/pilot_1/sql.txt\n/kaggle/input/databased/test_database/network_2/schema.sql\n/kaggle/input/databased/test_database/network_2/network_2.sqlite\n/kaggle/input/databased/test_database/baseball_1/baseball_1.sqlite\n/kaggle/input/databased/test_database/baseball_1/schema.sql\n/kaggle/input/databased/test_database/candidate_poll/candidate_poll.sqlite\n/kaggle/input/databased/test_database/candidate_poll/schema.sql\n/kaggle/input/databased/test_database/scientist_1/schema.sql\n/kaggle/input/databased/test_database/scientist_1/scientist_1.sqlite\n/kaggle/input/databased/test_database/wrestler/schema.sql\n/kaggle/input/databased/test_database/wrestler/wrestler.sqlite\n/kaggle/input/databased/test_database/dog_kennels/schema.sql\n/kaggle/input/databased/test_database/dog_kennels/dog_kennels.sqlite\n/kaggle/input/databased/test_database/customers_campaigns_ecommerce/schema.sql\n/kaggle/input/databased/test_database/customers_campaigns_ecommerce/customers_campaigns_ecommerce.sqlite\n/kaggle/input/databased/test_database/performance_attendance/performance_attendance.sqlite\n/kaggle/input/databased/test_database/performance_attendance/schema.sql\n/kaggle/input/databased/test_database/book_review/schema_old.sql\n/kaggle/input/databased/test_database/book_review/book_review.sqlite\n/kaggle/input/databased/test_database/book_review/schema.sql\n/kaggle/input/databased/test_database/online_exams/simple_schema.sql\n/kaggle/input/databased/test_database/online_exams/schema.sql\n/kaggle/input/databased/test_database/online_exams/online_exams.sqlite\n/kaggle/input/databased/test_database/device/device.sqlite\n/kaggle/input/databased/test_database/device/schema.sql\n/kaggle/input/databased/test_database/course_teach/course_teach.sqlite\n/kaggle/input/databased/test_database/course_teach/schema.sql\n/kaggle/input/databased/test_database/cre_Students_Information_Systems/cre_Students_Information_Systems.sqlite\n/kaggle/input/databased/test_database/cre_Students_Information_Systems/schema.sql\n/kaggle/input/databased/test_database/soccer_3/schema.sql\n/kaggle/input/databased/test_database/soccer_3/soccer_3.sqlite\n/kaggle/input/databased/test_database/gas_company/gas_company.sqlite\n/kaggle/input/databased/test_database/gas_company/schema.sql\n/kaggle/input/databased/test_database/university_rank/university_rank.sqlite\n/kaggle/input/databased/test_database/university_rank/schema.sql\n/kaggle/input/databased/test_database/imdb/imdb.sqlite\n/kaggle/input/databased/test_database/imdb/schema.sql\n/kaggle/input/databased/test_database/twitter_1/twitter_1.sqlite\n/kaggle/input/databased/test_database/twitter_1/queries/postgres-dialects.xml\n/kaggle/input/databased/test_database/twitter_1/queries/oracle-dialects.xml\n/kaggle/input/databased/test_database/twitter_1/queries/sqlserver-dialects.xml\n/kaggle/input/databased/test_database/flight_company/flight_company.sqlite\n/kaggle/input/databased/test_database/flight_company/schema.sql\n/kaggle/input/databased/test_database/hr_1/schema.sql\n/kaggle/input/databased/test_database/hr_1/hr_1.sqlite\n/kaggle/input/databased/test_database/swimming/swimming.sqlite\n/kaggle/input/databased/test_database/swimming/schema.sql\n/kaggle/input/databased/test_database/sing_contest/schema_old.sql\n/kaggle/input/databased/test_database/sing_contest/sing_contest.sqlite\n/kaggle/input/databased/test_database/sing_contest/schema.sql\n/kaggle/input/databased/test_database/architecture/architecture.sqlite\n/kaggle/input/databased/test_database/architecture/schema.sql\n/kaggle/input/databased/test_database/world_1/world_1.json\n/kaggle/input/databased/test_database/world_1/world_1.sqlite\n/kaggle/input/databased/test_database/bakery_1/bakery_1.sql\n/kaggle/input/databased/test_database/bakery_1/q.txt\n/kaggle/input/databased/test_database/bakery_1/bakery_1.json\n/kaggle/input/databased/test_database/bakery_1/link.txt\n/kaggle/input/databased/test_database/bakery_1/bakery_1.sqlite\n/kaggle/input/databased/test_database/bakery_1/bakery_1_michi.txt\n/kaggle/input/databased/test_database/bakery_1/annotation.json\n/kaggle/input/databased/test_database/bakery_1/data_csv/customers_t.csv\n/kaggle/input/databased/test_database/bakery_1/data_csv/items.csv\n/kaggle/input/databased/test_database/bakery_1/data_csv/receipts_t.csv\n/kaggle/input/databased/test_database/bakery_1/data_csv/items_t.csv\n/kaggle/input/databased/test_database/bakery_1/data_csv/goods_t.csv\n/kaggle/input/databased/test_database/bakery_1/data_csv/items (3_11_18, 5_53 PM)_original.csv\n/kaggle/input/databased/test_database/bakery_1/data_csv/customers.csv\n/kaggle/input/databased/test_database/bakery_1/data_csv/goods.csv\n/kaggle/input/databased/test_database/bakery_1/data_csv/README.BAKERY.TXT\n/kaggle/input/databased/test_database/bakery_1/data_csv/receipts.csv\n/kaggle/input/databased/test_database/bakery_1/data_csv/receipts (3_11_18, 5_53 PM)_original.csv\n/kaggle/input/databased/test_database/cre_Doc_and_collections/schema.sql\n/kaggle/input/databased/test_database/cre_Doc_and_collections/cre_Doc_and_collections.sqlite\n/kaggle/input/databased/test_database/match_season/schema.sql\n/kaggle/input/databased/test_database/match_season/match_season.sqlite\n/kaggle/input/databased/test_database/real_estate_rentals/real_estate_rentals.sqlite\n/kaggle/input/databased/test_database/real_estate_rentals/schema.sql\n/kaggle/input/databased/test_database/soccer_1/soccer_1.sqlite\n/kaggle/input/databased/test_database/soccer_1/schema.sql\n/kaggle/input/databased/test_database/formula_1/formula_1.splite\n/kaggle/input/databased/test_database/formula_1/annotation.json\n/kaggle/input/databased/test_database/formula_1/formula_1.sqlite\n/kaggle/input/databased/test_database/formula_1/formula_1.sql\n/kaggle/input/databased/test_database/formula_1/data_csv/races.csv\n/kaggle/input/databased/test_database/formula_1/data_csv/drivers.csv\n/kaggle/input/databased/test_database/formula_1/data_csv/constructors.csv\n/kaggle/input/databased/test_database/formula_1/data_csv/pitStops.csv\n/kaggle/input/databased/test_database/formula_1/data_csv/status.csv\n/kaggle/input/databased/test_database/formula_1/data_csv/seasons.csv\n/kaggle/input/databased/test_database/formula_1/data_csv/constructorStandings.csv\n/kaggle/input/databased/test_database/formula_1/data_csv/driverStandings.csv\n/kaggle/input/databased/test_database/formula_1/data_csv/constructorResults.csv\n/kaggle/input/databased/test_database/formula_1/data_csv/lapTimes.csv\n/kaggle/input/databased/test_database/formula_1/data_csv/results.csv\n/kaggle/input/databased/test_database/formula_1/data_csv/circuits.csv\n/kaggle/input/databased/test_database/formula_1/data_csv/qualifying.csv\n/kaggle/input/databased/test_database/car_1/car_1.json\n/kaggle/input/databased/test_database/car_1/q.txt\n/kaggle/input/databased/test_database/car_1/car_1.sqlite\n/kaggle/input/databased/test_database/car_1/link.txt\n/kaggle/input/databased/test_database/car_1/annotation.json\n/kaggle/input/databased/test_database/car_1/car_1.sql\n/kaggle/input/databased/test_database/car_1/data_csv/countries.csv\n/kaggle/input/databased/test_database/car_1/data_csv/cars-data.csv\n/kaggle/input/databased/test_database/car_1/data_csv/model-list.csv\n/kaggle/input/databased/test_database/car_1/data_csv/continents.csv\n/kaggle/input/databased/test_database/car_1/data_csv/car-makers.csv\n/kaggle/input/databased/test_database/car_1/data_csv/car-names.csv\n/kaggle/input/databased/test_database/car_1/data_csv/cars.desc\n/kaggle/input/databased/test_database/car_1/data_csv/README.CARS.TXT\n/kaggle/input/databased/test_database/university_basketball/schema.sql\n/kaggle/input/databased/test_database/university_basketball/university_basketball.sqlite\n/kaggle/input/databased/test_database/body_builder/body_builder.sqlite\n/kaggle/input/databased/test_database/body_builder/schema.sql\n/kaggle/input/databased/test_database/railway/schema.sql\n/kaggle/input/databased/test_database/railway/railway.sqlite\n/kaggle/input/databased/test_database/medicine_enzyme_interaction/medicine_enzyme_interaction.sqlite\n/kaggle/input/databased/test_database/medicine_enzyme_interaction/schema.sql\n/kaggle/input/databased/test_database/tracking_share_transactions/tracking_share_transactions.sqlite\n/kaggle/input/databased/test_database/tracking_share_transactions/schema.sql\n/kaggle/input/databased/test_database/musical/schema.sql\n/kaggle/input/databased/test_database/musical/musical.sqlite\n/kaggle/input/databased/test_database/tracking_software_problems/tracking_software_problems.sqlite\n/kaggle/input/databased/test_database/tracking_software_problems/schema.sql\n/kaggle/input/databased/test_database/academic/academic.sqlite\n/kaggle/input/databased/test_database/academic/schema.sql\n/kaggle/input/databased/test_database/products_for_hire/schema.sql\n/kaggle/input/databased/test_database/products_for_hire/products_for_hire.sqlite\n/kaggle/input/databased/test_database/roller_coaster/roller_coaster.sqlite\n/kaggle/input/databased/test_database/roller_coaster/schema.sql\n/kaggle/input/databased/test_database/conference/conference.sqlite\n/kaggle/input/databased/test_database/conference/schema.sql\n/kaggle/input/databased/test_database/tvshow/tvshow.sqlite\n/kaggle/input/databased/test_database/tvshow/schema.sql\n/kaggle/input/databased/test_database/college_3/schema.sql\n/kaggle/input/databased/test_database/college_3/college_3.sqlite\n/kaggle/input/databased/test_database/headphone_store/headphone_store.sqlite\n/kaggle/input/databased/test_database/headphone_store/schema.sql\n/kaggle/input/databased/test_database/bbc_channels/bbc_channels.sqlite\n/kaggle/input/databased/test_database/bbc_channels/schema.sql\n/kaggle/input/databased/test_database/cinema/cinema.sqlite\n/kaggle/input/databased/test_database/cinema/schema.sql\n/kaggle/input/databased/test_database/college_2/link.txt\n/kaggle/input/databased/test_database/college_2/college_2.sqlite\n/kaggle/input/databased/test_database/college_2/TextBookExampleSchema.sql\n/kaggle/input/databased/test_database/tracking_orders/schema.sql\n/kaggle/input/databased/test_database/tracking_orders/tracking_orders.sqlite\n/kaggle/input/databased/test_database/school_bus/school_bus.sqlite\n/kaggle/input/databased/test_database/school_bus/schema.sql\n/kaggle/input/databased/test_database/riding_club/riding_club.sqlite\n/kaggle/input/databased/test_database/riding_club/schema.sql\n/kaggle/input/databased/test_database/geo/geo.sqlite\n/kaggle/input/databased/test_database/geo/schema.sql\n/kaggle/input/databased/test_database/bike_1/bike_1.sqlite\n/kaggle/input/databased/test_database/bike_1/schema.sql\n/kaggle/input/databased/test_database/phone_1/phone_1.sqlite\n/kaggle/input/databased/test_database/phone_1/schema.sql\n/kaggle/input/databased/test_database/wine_1/q.txt\n/kaggle/input/databased/test_database/wine_1/link.txt\n/kaggle/input/databased/test_database/wine_1/wine_1.sqlite\n/kaggle/input/databased/test_database/wine_1/wine_1.sql\n/kaggle/input/databased/test_database/wine_1/annotation.json\n/kaggle/input/databased/test_database/wine_1/data_csv/grapes.csv\n/kaggle/input/databased/test_database/wine_1/data_csv/README.WINE.txt\n/kaggle/input/databased/test_database/wine_1/data_csv/appellations.csv\n/kaggle/input/databased/test_database/wine_1/data_csv/wine.csv\n/kaggle/input/databased/test_database/flight_2/flight_2.json\n/kaggle/input/databased/test_database/flight_2/q.txt\n/kaggle/input/databased/test_database/flight_2/flight_2.sqlite\n/kaggle/input/databased/test_database/flight_2/link.txt\n/kaggle/input/databased/test_database/flight_2/annotation.json\n/kaggle/input/databased/test_database/flight_2/flight_2.sql\n/kaggle/input/databased/test_database/flight_2/data_csv/airports100.csv\n/kaggle/input/databased/test_database/flight_2/data_csv/airlines.csv\n/kaggle/input/databased/test_database/flight_2/data_csv/README.AIRLINES.txt\n/kaggle/input/databased/test_database/flight_2/data_csv/flights.csv\n/kaggle/input/databased/test_database/soccer_2/schema.sql\n/kaggle/input/databased/test_database/soccer_2/soccer_2.sqlite\n/kaggle/input/databased/test_database/customers_and_addresses/schema.sql\n/kaggle/input/databased/test_database/customers_and_addresses/customers_and_addresses.sqlite\n/kaggle/input/databased/test_database/chinook_1/chinook_1.sqlite\n/kaggle/input/databased/test_database/chinook_1/annotation.json\n/kaggle/input/databased/test_database/station_weather/station_weather.sqlite\n/kaggle/input/databased/test_database/station_weather/schema.sql\n/kaggle/input/databased/test_database/insurance_fnol/insurance_fnol.sqlite\n/kaggle/input/databased/test_database/insurance_fnol/schema.sql\n/kaggle/input/databased/test_database/student_1/q.txt\n/kaggle/input/databased/test_database/student_1/link.txt\n/kaggle/input/databased/test_database/student_1/annotation.json\n/kaggle/input/databased/test_database/student_1/student_1.sqlite\n/kaggle/input/databased/test_database/student_1/student_1.sql\n/kaggle/input/databased/test_database/student_1/data_csv/list.csv\n/kaggle/input/databased/test_database/student_1/data_csv/teachers.csv\n/kaggle/input/databased/test_database/student_1/data_csv/README.STUDENTS.TXT\n/kaggle/input/databased/test_database/car_racing/car_racing.sqlite\n/kaggle/input/databased/test_database/car_racing/schema.sql\n/kaggle/input/databased/test_database/cre_Doc_Control_Systems/cre_Doc_Control_Systems.sqlite\n/kaggle/input/databased/test_database/cre_Doc_Control_Systems/schema.sql\n/kaggle/input/databased/test_database/school_finance/school_finance.sqlite\n/kaggle/input/databased/test_database/school_finance/schema.sql\n/kaggle/input/databased/test_database/singer/singer.sqlite\n/kaggle/input/databased/test_database/singer/schema.sql\n/kaggle/input/databased/test_database/advertising_agencies/schema.sql\n/kaggle/input/databased/test_database/advertising_agencies/advertising_agencies.sqlite\n/kaggle/input/databased/test_database/entrepreneur/entrepreneur.sqlite\n/kaggle/input/databased/test_database/entrepreneur/schema.sql\n/kaggle/input/databased/test_database/game_1/schema.sql\n/kaggle/input/databased/test_database/game_1/game_1.sqlite\n/kaggle/input/databased/test_database/scholar/schema.sql\n/kaggle/input/databased/test_database/scholar/scholar.sqlite\n/kaggle/input/databased/test_database/aan_1/schema.sql\n/kaggle/input/databased/test_database/aan_1/annotation.json\n/kaggle/input/databased/test_database/aan_1/aan_1.sqlite\n/kaggle/input/databased/test_database/customer_complaints/schema.sql\n/kaggle/input/databased/test_database/customer_complaints/customer_complaints.sqlite\n/kaggle/input/databased/test_database/sakila_1/sakila_1.sqlite\n/kaggle/input/databased/test_database/sakila_1/schema.sql\n/kaggle/input/databased/test_database/voter_2/schema.sql\n/kaggle/input/databased/test_database/voter_2/voter_2.sqlite\n/kaggle/input/databased/test_database/pets_1/schema.sql\n/kaggle/input/databased/test_database/pets_1/pets_1.sqlite\n/kaggle/input/databased/test_database/music_1/music_1.sqlite\n/kaggle/input/databased/test_database/music_1/schema.sql\n/kaggle/input/databased/test_database/browser_web/browser_web.sqlite\n/kaggle/input/databased/test_database/browser_web/schema.sql\n/kaggle/input/databased/test_database/wta_1/wta_1.sqlite\n/kaggle/input/databased/test_database/wta_1/wta_1.sql\n/kaggle/input/databased/test_database/cre_Doc_Tracking_DB/cre_Doc_Tracking_DB.sqlite\n/kaggle/input/databased/test_database/cre_Doc_Tracking_DB/schema.sql\n/kaggle/input/databased/test_database/cre_Theme_park/cre_Theme_park.sqlite\n/kaggle/input/databased/test_database/cre_Theme_park/schema.sql\n/kaggle/input/databased/test_database/e_learning/schema.sql\n/kaggle/input/databased/test_database/e_learning/e_learning.sqlite\n/kaggle/input/databased/test_database/bike_racing/schema_old.sql\n/kaggle/input/databased/test_database/bike_racing/bike_racing.sqlite\n/kaggle/input/databased/test_database/bike_racing/schema.sql\n/kaggle/input/databased/test_database/school_player/school_player.sqlite\n/kaggle/input/databased/test_database/school_player/schema.sql\n/kaggle/input/databased/test_database/movie_1/movie_1.sqlite\n/kaggle/input/databased/test_database/movie_1/schema.sql\n/kaggle/input/databased/test_database/flight_4/flight_4.sqlite\n/kaggle/input/databased/test_database/flight_4/link.txt\n/kaggle/input/databased/test_database/flight_4/sql.txt\n/kaggle/input/databased/test_database/activity_1/activity_1.sqlite\n/kaggle/input/databased/test_database/activity_1/schema.sql\n/kaggle/input/databased/test_database/perpetrator/perpetrator.sqlite\n/kaggle/input/databased/test_database/perpetrator/schema.sql\n/kaggle/input/databased/test_database/race_track/race_track.sqlite\n/kaggle/input/databased/test_database/race_track/schema.sql\n/kaggle/input/databased/test_database/decoration_competition/decoration_competition.sqlite\n/kaggle/input/databased/test_database/decoration_competition/schema.sql\n/kaggle/input/databased/test_database/movie_2/q.txt\n/kaggle/input/databased/test_database/movie_2/link.txt\n/kaggle/input/databased/test_database/movie_2/schema.sql\n/kaggle/input/databased/test_database/movie_2/movie_2.sqlite\n/kaggle/input/databased/test_database/movie_2/annotation.json\n/kaggle/input/databased/test_database/movie_2/sql.txt\n/kaggle/input/databased/test_database/employee_hire_evaluation/schema.sql\n/kaggle/input/databased/test_database/employee_hire_evaluation/employee_hire_evaluation.sqlite\n/kaggle/input/databased/test_database/ship_1/ship_1.sqlite\n/kaggle/input/databased/test_database/ship_1/schema.sql\n/kaggle/input/databased/test_database/restaurant_bills/schema_old.sql\n/kaggle/input/databased/test_database/restaurant_bills/schema.sql\n/kaggle/input/databased/test_database/restaurant_bills/restaurant_bills.sqlite\n/kaggle/input/databased/test_database/book_press/book_press.sqlite\n/kaggle/input/databased/test_database/book_press/schema.sql\n/kaggle/input/databased/test_database/election/election.sqlite\n/kaggle/input/databased/test_database/election/schema.sql\n/kaggle/input/databased/test_database/aircraft/aircraft.sqlite\n/kaggle/input/databased/test_database/aircraft/schema.sql\n/kaggle/input/databased/test_database/theme_gallery/schema.sql\n/kaggle/input/databased/test_database/theme_gallery/theme_gallery.sqlite\n/kaggle/input/databased/test_database/department_management/department_management.sqlite\n/kaggle/input/databased/test_database/department_management/schema.sql\n/kaggle/input/databased/test_database/address_1/address_1.sqlite\n/kaggle/input/databased/test_database/address_1/link.txt\n/kaggle/input/databased/test_database/address_1/schema.sql\n/kaggle/input/databased/test_database/product_catalog/product_catalog.sqlite\n/kaggle/input/databased/test_database/product_catalog/schema.sql\n/kaggle/input/databased/test_database/department_store/department_store.sqlite\n/kaggle/input/databased/test_database/department_store/schema.sql\n/kaggle/input/databased/test_database/district_spokesman/district_spokesman.sqlite\n/kaggle/input/databased/test_database/district_spokesman/schema.sql\n/kaggle/input/databased/test_database/college_1/college_1.sqlite\n/kaggle/input/databased/test_database/college_1/TinyCollege.sql\n/kaggle/input/databased/test_database/college_1/link.txt\n/kaggle/input/databased/test_database/workshop_paper/workshop_paper.sqlite\n/kaggle/input/databased/test_database/workshop_paper/schema.sql\n/kaggle/input/databased/test_database/tv_shows/schema.sql\n/kaggle/input/databased/test_database/tv_shows/tv_shows.sqlite\n/kaggle/input/databased/test_database/protein_institute/protein_institute.sqlite\n/kaggle/input/databased/test_database/protein_institute/schema.sql\n/kaggle/input/databased/test_database/network_1/network_1.sqlite\n/kaggle/input/databased/test_database/network_1/schema.sql\n/kaggle/input/databased/test_database/game_injury/game_injury.sqlite\n/kaggle/input/databased/test_database/game_injury/schema.sql\n/kaggle/input/databased/test_database/e_government/e_government.sqlite\n/kaggle/input/databased/test_database/e_government/schema.sql\n/kaggle/input/databased/test_database/dorm_1/schema.sql\n/kaggle/input/databased/test_database/dorm_1/dorm_1.sqlite\n/kaggle/input/databased/test_database/store_product/store_product.sqlite\n/kaggle/input/databased/test_database/store_product/schema.sql\n/kaggle/input/databased/test_database/cre_Doc_Workflow/cre_Doc_Workflow.sqlite\n/kaggle/input/databased/test_database/cre_Doc_Workflow/schema.sql\n/kaggle/input/databased/test_database/cre_Docs_and_Epenses/cre_Docs_and_Epenses.sqlite\n/kaggle/input/databased/test_database/cre_Docs_and_Epenses/schema.sql\n/kaggle/input/databased/test_database/club_1/club_1.sqlite\n/kaggle/input/databased/test_database/club_1/schema.sql\n/kaggle/input/databased/test_database/small_bank_1/small_bank_1.sqlite\n/kaggle/input/databased/test_database/program_share/schema.sql\n/kaggle/input/databased/test_database/program_share/program_share.sqlite\n/kaggle/input/databased/test_database/e_commerce/schema.sql\n/kaggle/input/databased/test_database/e_commerce/e_commerce.sqlite\n/kaggle/input/databased/test_database/cre_Drama_Workshop_Groups/cre_Drama_Workshop_Groups.sqlite\n/kaggle/input/databased/test_database/cre_Drama_Workshop_Groups/schema.sql\n/kaggle/input/databased/test_database/phone_market/phone_market.sqlite\n/kaggle/input/databased/test_database/phone_market/schema.sql\n/kaggle/input/databased/test_database/company_office/schema.sql\n/kaggle/input/databased/test_database/company_office/company_office.sqlite\n/kaggle/input/databased/test_database/local_govt_in_alabama/local_govt_in_alabama.sqlite\n/kaggle/input/databased/test_database/local_govt_in_alabama/schema.sql\n/kaggle/input/databased/test_database/store_1/schema.sql\n/kaggle/input/databased/test_database/store_1/store_1.sqlite\n/kaggle/input/databased/test_database/company_1/company_1.sqlite\n/kaggle/input/databased/test_database/company_1/link.txt\n/kaggle/input/databased/test_database/debate/schema.sql\n/kaggle/input/databased/test_database/debate/debate.sqlite\n/kaggle/input/databased/test_database/vehicle_rent/vehicle_rent.sqlite\n/kaggle/input/databased/test_database/vehicle_rent/schema.sql\n/kaggle/input/databased/test_database/vehicle_rent/vehicle_rent\n/kaggle/input/databased/test_database/battle_death/battle_death.sqlite\n/kaggle/input/databased/test_database/battle_death/schema.sql\n/kaggle/input/databased/test_database/customers_card_transactions/schema.sql\n/kaggle/input/databased/test_database/customers_card_transactions/customers_card_transactions.sqlite\n/kaggle/input/databased/test_database/music_2/music_2.sqlite\n/kaggle/input/databased/test_database/music_2/schema.sql\n/kaggle/input/databased/test_database/customer_deliveries/customer_deliveries.sqlite\n/kaggle/input/databased/test_database/customer_deliveries/schema.sql\n/kaggle/input/databased/test_database/poker_player/schema.sql\n/kaggle/input/databased/test_database/poker_player/poker_player.sqlite\n/kaggle/input/databased/test_database/region_building/region_building.sqlite\n/kaggle/input/databased/test_database/region_building/schema.sql\n/kaggle/input/databased/test_database/coffee_shop/schema.sql\n/kaggle/input/databased/test_database/coffee_shop/coffee_shop.sqlite\n/kaggle/input/databased/test_database/manufactory_1/schema.sql\n/kaggle/input/databased/test_database/manufactory_1/manufactory_1.sqlite\n/kaggle/input/databased/test_database/allergy_1/allergy_1.sqlite\n/kaggle/input/databased/test_database/allergy_1/schema.sql\n/kaggle/input/databased/test_database/music_4/schema.sql\n/kaggle/input/databased/test_database/music_4/music_4.sqlite\n/kaggle/input/databased/test_database/document_management/document_management.sqlite\n/kaggle/input/databased/test_database/document_management/schema.sql\n/kaggle/input/databased/test_database/boat_1/Boats.csv\n/kaggle/input/databased/test_database/boat_1/Reserves.csv\n/kaggle/input/databased/test_database/boat_1/schema.sql\n/kaggle/input/databased/test_database/boat_1/boat_1.sqlite\n/kaggle/input/databased/test_database/boat_1/Sailors.csv\n/kaggle/input/databased/test_database/hospital_1/schema.sql\n/kaggle/input/databased/test_database/hospital_1/hospital_1.sqlite\n/kaggle/input/databased/test_database/video_game/schema.sql\n/kaggle/input/databased/test_database/video_game/video_game.sqlite\n/kaggle/input/databased/test_database/art_1/q.txt\n/kaggle/input/databased/test_database/art_1/link.txt\n/kaggle/input/databased/test_database/art_1/art_1.sqlite\n/kaggle/input/databased/test_database/student_assessment/student_assessment.sqlite\n/kaggle/input/databased/test_database/student_assessment/schema.sql\n/kaggle/input/databased/test_database/sports_competition/schema.sql\n/kaggle/input/databased/test_database/sports_competition/sports_competition.sqlite\n/kaggle/input/databased/test_database/journal_committee/journal_committee.sqlite\n/kaggle/input/databased/test_database/journal_committee/schema.sql\n/kaggle/input/databased/test_database/farm/schema.sql\n/kaggle/input/databased/test_database/farm/farm.sqlite\n/kaggle/input/databased/test_database/book_2/book_2.sqlite\n/kaggle/input/databased/test_database/book_2/schema.sql\n/kaggle/input/databased/test_database/climbing/climbing.sqlite\n/kaggle/input/databased/test_database/climbing/schema.sql\n/kaggle/input/databased/test_database/real_estate_properties/schema.sql\n/kaggle/input/databased/test_database/real_estate_properties/real_estate_properties.sqlite\n/kaggle/input/databased/test_database/news_report/news_report.sqlite\n/kaggle/input/databased/test_database/news_report/schema.sql\n/kaggle/input/databased/test_database/customers_and_invoices/customers_and_invoices.sqlite\n/kaggle/input/databased/test_database/customers_and_invoices/schema.sql\n/kaggle/input/databased/test_database/shop_membership/schema.sql\n/kaggle/input/databased/test_database/shop_membership/shop_membership.sqlite\n/kaggle/input/databased/test_database/party_people/party_people.sqlite\n/kaggle/input/databased/test_database/party_people/schema.sql\n/kaggle/input/databased/test_database/club_leader/club_leader.sqlite\n/kaggle/input/databased/test_database/club_leader/schema_old.sql\n/kaggle/input/databased/test_database/club_leader/schema.sql\n/kaggle/input/databased/test_database/car_road_race/schema.sql\n/kaggle/input/databased/test_database/car_road_race/car_road_race.sqlite\n/kaggle/input/databased/test_database/vehicle_driver/schema.sql\n/kaggle/input/databased/test_database/vehicle_driver/vehicle_driver.sqlite\n/kaggle/input/databased/test_database/museum_visit/schema.sql\n/kaggle/input/databased/test_database/museum_visit/museum_visit.sqlite\n/kaggle/input/databased/test_database/icfp_1/q.txt\n/kaggle/input/databased/test_database/icfp_1/link.txt\n/kaggle/input/databased/test_database/icfp_1/icfp_1.sqlite\n/kaggle/input/databased/test_database/student_transcripts_tracking/schema.sql\n/kaggle/input/databased/test_database/student_transcripts_tracking/student_transcripts_tracking.sqlite\n/kaggle/input/databased/test_database/products_gen_characteristics/products_gen_characteristics.sqlite\n/kaggle/input/databased/test_database/products_gen_characteristics/schema.sql\n/kaggle/input/databased/test_database/institution_sports/institution_sports.sqlite\n/kaggle/input/databased/test_database/institution_sports/schema_old.sql\n/kaggle/input/databased/test_database/institution_sports/schema.sql\n/kaggle/input/spider-data/test_tables.json\n/kaggle/input/spider-data/train_spider.json\n/kaggle/input/spider-data/train_others.json\n/kaggle/input/spider-data/dev_gold.sql\n/kaggle/input/spider-data/deepspeed.json\n/kaggle/input/spider-data/dev.json\n/kaggle/input/spider-data/README.txt\n/kaggle/input/spider-data/test.json\n/kaggle/input/spider-data/train_gold.sql\n/kaggle/input/spider-data/tables.json\n/kaggle/input/spider-data/test_gold.sql\n/kaggle/input/preprocessed/preprocessed_val_dataset.json\n/kaggle/input/preprocessed/preprocessed_val_dataset_nat.json\n/kaggle/input/preprocessed/preprocessed_train_dataset.json\n/kaggle/input/preprocessed/preprocessed_train_dataset_nat.json\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"!pip install rapidfuzz\n\nimport difflib\nfrom typing import List, Optional, Tuple\nfrom rapidfuzz import fuzz\nimport sqlite3\nimport functools\n\n# fmt: off\n_stopwords = {'who', 'ourselves', 'down', 'only', 'were', 'him', 'at', \"weren't\", 'has', 'few', \"it's\", 'm', 'again',\n              'd', 'haven', 'been', 'other', 'we', 'an', 'own', 'doing', 'ma', 'hers', 'all', \"haven't\", 'in', 'but',\n              \"shouldn't\", 'does', 'out', 'aren', 'you', \"you'd\", 'himself', \"isn't\", 'most', 'y', 'below', 'is',\n              \"wasn't\", 'hasn', 'them', 'wouldn', 'against', 'this', 'about', 'there', 'don', \"that'll\", 'a', 'being',\n              'with', 'your', 'theirs', 'its', 'any', 'why', 'now', 'during', 'weren', 'if', 'should', 'those', 'be',\n              'they', 'o', 't', 'of', 'or', 'me', 'i', 'some', 'her', 'do', 'will', 'yours', 'for', 'mightn', 'nor',\n              'needn', 'the', 'until', \"couldn't\", 'he', 'which', 'yourself', 'to', \"needn't\", \"you're\", 'because',\n              'their', 'where', 'it', \"didn't\", 've', 'whom', \"should've\", 'can', \"shan't\", 'on', 'had', 'have',\n              'myself', 'am', \"don't\", 'under', 'was', \"won't\", 'these', 'so', 'as', 'after', 'above', 'each', 'ours',\n              'hadn', 'having', 'wasn', 's', 'doesn', \"hadn't\", 'than', 'by', 'that', 'both', 'herself', 'his',\n              \"wouldn't\", 'into', \"doesn't\", 'before', 'my', 'won', 'more', 'are', 'through', 'same', 'how', 'what',\n              'over', 'll', 'yourselves', 'up', 'mustn', \"mustn't\", \"she's\", 're', 'such', 'didn', \"you'll\", 'shan',\n              'when', \"you've\", 'themselves', \"mightn't\", 'she', 'from', 'isn', 'ain', 'between', 'once', 'here',\n              'shouldn', 'our', 'and', 'not', 'too', 'very', 'further', 'while', 'off', 'couldn', \"hasn't\", 'itself',\n              'then', 'did', 'just', \"aren't\"}\n# fmt: on\n\n_commonwords = {\"no\", \"yes\", \"many\"}\n\n\ndef is_number(s: str) -> bool:\n    try:\n        float(s.replace(\",\", \"\"))\n        return True\n    except:\n        return False\n\n\ndef is_stopword(s: str) -> bool:\n    return s.strip() in _stopwords\n\n\ndef is_commonword(s: str) -> bool:\n    return s.strip() in _commonwords\n\n\ndef is_common_db_term(s: str) -> bool:\n    return s.strip() in [\"id\"]\n\n\nclass Match(object):\n    def __init__(self, start: int, size: int) -> None:\n        self.start = start\n        self.size = size\n\n\ndef is_span_separator(c: str) -> bool:\n    return c in \"'\\\"()`,.?! \"\n\n\ndef split(s: str) -> List[str]:\n    return [c.lower() for c in s.strip()]\n\n\ndef prefix_match(s1: str, s2: str) -> bool:\n    i, j = 0, 0\n    for i in range(len(s1)):\n        if not is_span_separator(s1[i]):\n            break\n    for j in range(len(s2)):\n        if not is_span_separator(s2[j]):\n            break\n    if i < len(s1) and j < len(s2):\n        return s1[i] == s2[j]\n    elif i >= len(s1) and j >= len(s2):\n        return True\n    else:\n        return False\n\n\ndef get_effective_match_source(s: str, start: int, end: int) -> Match:\n    _start = -1\n\n    for i in range(start, start - 2, -1):\n        if i < 0:\n            _start = i + 1\n            break\n        if is_span_separator(s[i]):\n            _start = i\n            break\n\n    if _start < 0:\n        return None\n\n    _end = -1\n    for i in range(end - 1, end + 3):\n        if i >= len(s):\n            _end = i - 1\n            break\n        if is_span_separator(s[i]):\n            _end = i\n            break\n\n    if _end < 0:\n        return None\n\n    while _start < len(s) and is_span_separator(s[_start]):\n        _start += 1\n    while _end >= 0 and is_span_separator(s[_end]):\n        _end -= 1\n\n    return Match(_start, _end - _start + 1)\n\n\ndef get_matched_entries(\n    s: str, field_values: List[str], m_theta: float = 0.85, s_theta: float = 0.85\n) -> Optional[List[Tuple[str, Tuple[str, str, float, float, int]]]]:\n    if not field_values:\n        return None\n\n    if isinstance(s, str):\n        n_grams = split(s)\n    else:\n        n_grams = s\n\n    matched = dict()\n    for field_value in field_values:\n        if not isinstance(field_value, str):\n            continue\n        fv_tokens = split(field_value)\n        sm = difflib.SequenceMatcher(None, n_grams, fv_tokens)\n        match = sm.find_longest_match(0, len(n_grams), 0, len(fv_tokens))\n        if match.size > 0:\n            source_match = get_effective_match_source(\n                n_grams, match.a, match.a + match.size\n            )\n            if source_match and source_match.size > 1:\n                match_str = field_value[match.b : match.b + match.size]\n                source_match_str = s[\n                    source_match.start : source_match.start + source_match.size\n                ]\n                c_match_str = match_str.lower().strip()\n                c_source_match_str = source_match_str.lower().strip()\n                c_field_value = field_value.lower().strip()\n                if (\n                    c_match_str\n                    and not is_number(c_match_str)\n                    and not is_common_db_term(c_match_str)\n                ):\n                    if (\n                        is_stopword(c_match_str)\n                        or is_stopword(c_source_match_str)\n                        or is_stopword(c_field_value)\n                    ):\n                        continue\n                    if c_source_match_str.endswith(c_match_str + \"'s\"):\n                        match_score = 1.0\n                    else:\n                        if prefix_match(c_field_value, c_source_match_str):\n                            match_score = (\n                                fuzz.ratio(c_field_value, c_source_match_str) / 100\n                            )\n                        else:\n                            match_score = 0\n                    if (\n                        is_commonword(c_match_str)\n                        or is_commonword(c_source_match_str)\n                        or is_commonword(c_field_value)\n                    ) and match_score < 1:\n                        continue\n                    s_match_score = match_score\n                    if match_score >= m_theta and s_match_score >= s_theta:\n                        if field_value.isupper() and match_score * s_match_score < 1:\n                            continue\n                        matched[match_str] = (\n                            field_value,\n                            source_match_str,\n                            match_score,\n                            s_match_score,\n                            match.size,\n                        )\n    \n    if not matched:\n        return None\n    else:\n        return sorted(\n            matched.items(),\n            key=lambda x: (1e16 * x[1][2] + 1e8 * x[1][3] + x[1][4]),\n            reverse=True,\n        )\n\n\n@functools.lru_cache(maxsize=1000, typed=False)\ndef get_column_picklist(table_name: str, column_name: str, db_path: str) -> list:\n    fetch_sql = \"SELECT DISTINCT `{}` FROM `{}`\".format(column_name, table_name)\n    try:\n        conn = sqlite3.connect(db_path)\n        conn.text_factory = bytes\n        c = conn.cursor()\n        c.execute(fetch_sql)\n        picklist = set()\n        for x in c.fetchall():\n            if isinstance(x[0], str):\n                picklist.add(x[0].encode(\"utf-8\"))\n            elif isinstance(x[0], bytes):\n                try:\n                    picklist.add(x[0].decode(\"utf-8\"))\n                except UnicodeDecodeError:\n                    picklist.add(x[0].decode(\"latin-1\"))\n            else:\n                picklist.add(x[0])\n        picklist = list(picklist)\n    except Exception as e:\n        picklist = []\n    finally:\n        conn.close()\n    return picklist\n\n\ndef get_database_matches(\n    question: str,\n    table_name: str,\n    column_name: str,\n    db_path: str,\n    top_k_matches: int = 2,\n    match_threshold: float = 0.85,\n) -> List[str]:\n    picklist = get_column_picklist(\n        table_name=table_name, column_name=column_name, db_path=db_path\n    )\n    # only maintain data in ``str'' type\n    picklist = [ele.strip() for ele in picklist if isinstance(ele, str)]\n    # picklist is unordered, we sort it to ensure the reproduction stability\n    picklist = sorted(picklist)\n    \n    matches = []\n    if picklist and isinstance(picklist[0], str):\n        matched_entries = get_matched_entries(\n            s=question,\n            field_values=picklist,\n            m_theta=match_threshold,\n            s_theta=match_threshold,\n        )\n\n        if matched_entries:\n            num_values_inserted = 0\n            for _match_str, (\n                field_value,\n                _s_match_str,\n                match_score,\n                s_match_score,\n                _match_size,\n            ) in matched_entries:\n                if \"name\" in column_name and match_score * s_match_score < 1:\n                    continue\n                if table_name != \"sqlite_sequence\":  # Spider database artifact\n                    matches.append(field_value.strip())\n                    num_values_inserted += 1\n                    if num_values_inserted >= top_k_matches:\n                        break\n    return matches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:57:13.865670Z","iopub.execute_input":"2025-05-10T04:57:13.865895Z","iopub.status.idle":"2025-05-10T04:57:16.881900Z","shell.execute_reply.started":"2025-05-10T04:57:13.865879Z","shell.execute_reply":"2025-05-10T04:57:16.881203Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (3.13.0)\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"from collections import defaultdict\n\ndef extract_columns(sql, column_names, table_names, aliases=None, from_table_idxs=None):\n    used_tables = defaultdict(set)\n    current_from_table_idxs = set()\n    alias_to_table_idx = {}\n\n    def resolve_col(col_unit):\n        if col_unit is None:\n            return\n        _, col_idx, _ = col_unit\n        if col_idx == 0:  # wildcard\n            for t_idx, col_name in column_names:\n                if t_idx in from_table_idxs:\n                    table_name = table_names[t_idx]\n                    used_tables[table_name].add(col_name)\n        elif col_idx > 0:\n            t_idx, col_name = column_names[col_idx]\n            if t_idx >= 0:\n                table_name = table_names[t_idx]\n                used_tables[table_name].add(col_name)\n\n    def resolve_val_unit(val_unit):\n        if val_unit is None:\n            return\n        _, col_unit1, col_unit2 = val_unit\n        resolve_col(col_unit1)\n        resolve_col(col_unit2)\n\n    def resolve_val(val):\n        if isinstance(val, dict):  # subquery\n            sub_from_idxs = get_from_table_idxs(val)\n            sub_used = extract_columns(val, column_names, table_names, {}, sub_from_idxs)\n            for tbl, cols in sub_used.items():\n                used_tables[tbl].update(cols)\n        elif isinstance(val, (tuple, list)) and len(val) == 3:\n            resolve_col(tuple(val))  # Convert list to tuple\n\n    def resolve_condition(conds):\n        if not conds:\n            return\n        if isinstance(conds[0], list):\n            for cond in conds:\n                if isinstance(cond, list) and len(cond) == 5:\n                    not_op, op_id, val_unit, val1, val2 = cond\n                    resolve_val_unit(val_unit)\n                    resolve_val(val1)\n                    resolve_val(val2)\n        else:\n            not_op, op_id, val_unit, val1, val2 = conds\n            resolve_val_unit(val_unit)\n            resolve_val(val1)\n            resolve_val(val2)\n\n    def resolve_table_units(table_units):\n        for unit in table_units:\n            type_, val = unit\n            if type_ == \"table_unit\":\n                if isinstance(val, int):\n                    current_from_table_idxs.add(val)\n                    table_name = table_names[val]\n                    alias_to_table_idx[str(val)] = val\n                elif isinstance(val, dict):  # subquery\n                    sub_from_idxs = get_from_table_idxs(val)\n                    sub_used = extract_columns(val, column_names, table_names, {}, sub_from_idxs)\n                    for tbl, cols in sub_used.items():\n                        used_tables[tbl].update(cols)\n            elif type_ == \"sql\":  # e.g., derived table\n                sub_from_idxs = get_from_table_idxs(val)\n                sub_used = extract_columns(val, column_names, table_names, {}, sub_from_idxs)\n                for tbl, cols in sub_used.items():\n                    used_tables[tbl].update(cols)\n\n    def get_from_table_idxs(sql_dict):\n        from_ = sql_dict.get(\"from\", {})\n        idxs = set()\n        for unit in from_.get(\"table_units\", []):\n            type_, val = unit\n            if type_ == \"table_unit\" and isinstance(val, int):\n                idxs.add(val)\n        return idxs\n\n    # FROM clause\n    from_ = sql.get(\"from\", {})\n    table_units = from_.get(\"table_units\", [])\n    resolve_table_units(table_units)\n    resolve_condition(from_.get(\"conds\", []))\n    \n    # ✅ Set this before using resolve_col\n    if from_table_idxs is None:\n        from_table_idxs = current_from_table_idxs\n    \n    # SELECT\n    select_clause = sql.get(\"select\", [False, []])\n    if select_clause and isinstance(select_clause, list):\n        _, select_cols = select_clause\n        for agg_id, val_unit in select_cols:\n            resolve_val_unit(val_unit)\n\n    # WHERE clause\n    resolve_condition(sql.get(\"where\", []))\n\n    # GROUP BY clause\n    for col_unit in sql.get(\"groupBy\", []):\n        resolve_col(col_unit)\n\n    # HAVING clause\n    resolve_condition(sql.get(\"having\", []))\n\n    # ORDER BY clause\n    order_by = sql.get(\"orderBy\")\n    if order_by:\n        _, val_units = order_by\n        for val_unit in val_units:\n            resolve_val_unit(val_unit)\n\n    # INTERSECT / UNION / EXCEPT\n    for key in [\"intersect\", \"union\", \"except\"]:\n        if sql.get(key):\n            sub_from_idxs = get_from_table_idxs(sql[key])\n            sub_used = extract_columns(sql[key], column_names, table_names, {}, sub_from_idxs)\n            for tbl, cols in sub_used.items():\n                used_tables[tbl].update(cols)\n\n    return used_tables\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:57:16.883189Z","iopub.execute_input":"2025-05-10T04:57:16.883898Z","iopub.status.idle":"2025-05-10T04:57:16.898763Z","shell.execute_reply.started":"2025-05-10T04:57:16.883859Z","shell.execute_reply":"2025-05-10T04:57:16.897894Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"def format_used_columns(used_tables):\n    return \" | \".join(f\"{table}: {' , '.join(sorted(cols))}\" for table, cols in used_tables.items())\n\ntables_path = \"/kaggle/input/spider-data/tables.json\"\ntrain_path = \"/kaggle/input/spider-data/train_spider.json\"\nwith open(tables_path, \"r\") as f:\n    tables = {tbl[\"db_id\"]: tbl for tbl in json.load(f)}\n\nwith open(train_path, \"r\") as f:\n    train = json.load(f)\n\nfor i, item in enumerate(train[40:50]):\n    sql = item[\"sql\"]\n    db_id = item[\"db_id\"]\n    query = item[\"query\"]\n    table_entry = tables[db_id]\n    column_names = table_entry[\"column_names_original\"]\n    table_names = table_entry[\"table_names_original\"]\n    aliases = {}\n\n    used = extract_columns(sql, column_names, table_names, aliases)\n    formatted = format_used_columns(used).lower()\n\n    print(f\"\\nExample {i+1} ({query}\\n{db_id}):\\n{formatted}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:57:16.900606Z","iopub.execute_input":"2025-05-10T04:57:16.900797Z","iopub.status.idle":"2025-05-10T04:57:17.142214Z","shell.execute_reply.started":"2025-05-10T04:57:16.900774Z","shell.execute_reply":"2025-05-10T04:57:17.141344Z"}},"outputs":[{"name":"stdout","text":"\nExample 1 (SELECT T2.Theme FROM city AS T1 JOIN farm_competition AS T2 ON T1.City_ID  =  T2.Host_city_ID WHERE T1.Population  >  1000\nfarm):\ncity: city_id , population | farm_competition: host_city_id , theme\n\nExample 2 (SELECT T2.Theme FROM city AS T1 JOIN farm_competition AS T2 ON T1.City_ID  =  T2.Host_city_ID WHERE T1.Population  >  1000\nfarm):\ncity: city_id , population | farm_competition: host_city_id , theme\n\nExample 3 (SELECT Status ,  avg(Population) FROM city GROUP BY Status\nfarm):\ncity: population , status\n\nExample 4 (SELECT Status ,  avg(Population) FROM city GROUP BY Status\nfarm):\ncity: population , status\n\nExample 5 (SELECT Status FROM city GROUP BY Status ORDER BY COUNT(*) ASC\nfarm):\ncity: area_km_2 , census_ranking , city_id , official_name , population , status\n\nExample 6 (SELECT Status FROM city GROUP BY Status ORDER BY COUNT(*) ASC\nfarm):\ncity: area_km_2 , census_ranking , city_id , official_name , population , status\n\nExample 7 (SELECT Status FROM city GROUP BY Status ORDER BY COUNT(*) DESC LIMIT 1\nfarm):\ncity: area_km_2 , census_ranking , city_id , official_name , population , status\n\nExample 8 (SELECT Status FROM city GROUP BY Status ORDER BY COUNT(*) DESC LIMIT 1\nfarm):\ncity: area_km_2 , census_ranking , city_id , official_name , population , status\n\nExample 9 (SELECT Official_Name FROM city WHERE City_ID NOT IN (SELECT Host_city_ID FROM farm_competition)\nfarm):\ncity: city_id , official_name | farm_competition: host_city_id\n\nExample 10 (SELECT Official_Name FROM city WHERE City_ID NOT IN (SELECT Host_city_ID FROM farm_competition)\nfarm):\ncity: city_id , official_name | farm_competition: host_city_id\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"def extract_skeleton(sql, db_schema):\n    table_names_original, table_dot_column_names_original, column_names_original = [], [], []\n    for table in db_schema[\"schema_items\"]:\n        table_name_original = table[\"table_name_original\"]\n        table_names_original.append(table_name_original)\n\n        for column_name_original in [\"*\"]+table[\"column_names_original\"]:\n            table_dot_column_names_original.append(table_name_original+\".\"+column_name_original)\n            column_names_original.append(column_name_original)\n    \n    parsed_sql = Parser(sql)\n    new_sql_tokens = []\n    for token in parsed_sql.tokens:\n        # mask table names\n        if token.value in table_names_original:\n            new_sql_tokens.append(\"_\")\n        # mask column names\n        elif token.value in column_names_original \\\n            or token.value in table_dot_column_names_original:\n            new_sql_tokens.append(\"_\")\n        # mask string values\n        elif token.value.startswith(\"'\") and token.value.endswith(\"'\"):\n            new_sql_tokens.append(\"_\")\n        # mask positive int number\n        elif token.value.isdigit():\n            new_sql_tokens.append(\"_\")\n        # mask negative int number\n        elif isNegativeInt(token.value):\n            new_sql_tokens.append(\"_\")\n        # mask float number\n        elif isFloat(token.value):\n            new_sql_tokens.append(\"_\")\n        else:\n            new_sql_tokens.append(token.value.strip())\n\n    sql_skeleton = \" \".join(new_sql_tokens)\n    \n    # remove JOIN ON keywords\n    sql_skeleton = sql_skeleton.replace(\"on _ = _ and _ = _\", \"on _ = _\")\n    sql_skeleton = sql_skeleton.replace(\"on _ = _ or _ = _\", \"on _ = _\")\n    sql_skeleton = sql_skeleton.replace(\" on _ = _\", \"\")\n    pattern3 = re.compile(\"_ (?:join _ ?)+\")\n    sql_skeleton = re.sub(pattern3, \"_ \", sql_skeleton)\n\n    # \"_ , _ , ..., _\" -> \"_\"\n    while(\"_ , _\" in sql_skeleton):\n        sql_skeleton = sql_skeleton.replace(\"_ , _\", \"_\")\n    \n    # remove clauses in WHERE keywords\n    ops = [\"=\", \"!=\", \">\", \">=\", \"<\", \"<=\"]\n    for op in ops:\n        if \"_ {} _\".format(op) in sql_skeleton:\n            sql_skeleton = sql_skeleton.replace(\"_ {} _\".format(op), \"_\")\n    while(\"where _ and _\" in sql_skeleton or \"where _ or _\" in sql_skeleton):\n        if \"where _ and _\"in sql_skeleton:\n            sql_skeleton = sql_skeleton.replace(\"where _ and _\", \"where _\")\n        if \"where _ or _\" in sql_skeleton:\n            sql_skeleton = sql_skeleton.replace(\"where _ or _\", \"where _\")\n\n    # remove additional spaces in the skeleton\n    while \"  \" in sql_skeleton:\n        sql_skeleton = sql_skeleton.replace(\"  \", \" \")\n\n    return sql_skeleton\n\n\ndef get_db_contents(question, table_name_original, column_names_original, db_id, db_path):\n    matched_contents = []\n    # extract matched contents for each column\n    for column_name_original in column_names_original:\n        matches = get_database_matches(\n            question, \n            table_name_original, \n            column_name_original, \n            db_path + \"/{}/{}.sqlite\".format(db_id, db_id)\n        )\n        matches = sorted(matches)\n        matched_contents.append(matches)\n    \n    return matched_contents\n\ndef get_db_schemas(all_db_infos):\n    db_schemas = {}\n\n    for db in all_db_infos:\n        table_names_original = db[\"table_names_original\"]\n        table_names = db[\"table_names\"]\n        column_names_original = db[\"column_names_original\"]\n        column_names = db[\"column_names\"]\n        column_types = db[\"column_types\"]\n\n        db_schemas[db[\"db_id\"]] = {}\n        \n        primary_keys, foreign_keys = [], []\n        # record primary keys\n        for pk_column_idx in db[\"primary_keys\"]:\n            pk_table_name_original = table_names_original[column_names_original[pk_column_idx][0]]\n            pk_column_name_original = column_names_original[pk_column_idx][1]\n            \n            primary_keys.append(\n                {\n                    \"table_name_original\": pk_table_name_original.lower(), \n                    \"column_name_original\": pk_column_name_original.lower()\n                }\n            )\n\n        db_schemas[db[\"db_id\"]][\"pk\"] = primary_keys\n\n        # record foreign keys\n        for source_column_idx, target_column_idx in db[\"foreign_keys\"]:\n            fk_source_table_name_original = table_names_original[column_names_original[source_column_idx][0]]\n            fk_source_column_name_original = column_names_original[source_column_idx][1]\n\n            fk_target_table_name_original = table_names_original[column_names_original[target_column_idx][0]]\n            fk_target_column_name_original = column_names_original[target_column_idx][1]\n            \n            foreign_keys.append(\n                {\n                    \"source_table_name_original\": fk_source_table_name_original.lower(),\n                    \"source_column_name_original\": fk_source_column_name_original.lower(),\n                    \"target_table_name_original\": fk_target_table_name_original.lower(),\n                    \"target_column_name_original\": fk_target_column_name_original.lower(),\n                }\n            )\n        db_schemas[db[\"db_id\"]][\"fk\"] = foreign_keys\n\n        db_schemas[db[\"db_id\"]][\"schema_items\"] = []\n        for idx, table_name_original in enumerate(table_names_original):\n            column_names_original_list = []\n            column_names_list = []\n            column_types_list = []\n            \n            for column_idx, (table_idx, column_name_original) in enumerate(column_names_original):\n                if idx == table_idx:\n                    column_names_original_list.append(column_name_original.lower())\n                    column_names_list.append(column_names[column_idx][1].lower())\n                    column_types_list.append(column_types[column_idx])\n            \n            db_schemas[db[\"db_id\"]][\"schema_items\"].append({\n                \"table_name_original\": table_name_original.lower(),\n                \"table_name\": table_names[idx].lower(), \n                \"column_names\": column_names_list, \n                \"column_names_original\": column_names_original_list,\n                \"column_types\": column_types_list\n            })\n\n    return db_schemas\n\n\ndef normalization(sql):\n    def white_space_fix(s):\n        parsed_s = Parser(s)\n        s = \" \".join([token.value for token in parsed_s.tokens])\n\n        return s\n\n    # convert everything except text between single quotation marks to lower case\n    def lower(s):\n        in_quotation = False\n        out_s = \"\"\n        for char in s:\n            if in_quotation:\n                out_s += char\n            else:\n                out_s += char.lower()\n            \n            if char == \"'\":\n                if in_quotation:\n                    in_quotation = False\n                else:\n                    in_quotation = True\n        \n        return out_s\n    \n    # remove \";\"\n    def remove_semicolon(s):\n        if s.endswith(\";\"):\n            s = s[:-1]\n        return s\n\n    # double quotation -> single quotation \n    def double2single(s):\n        return s.replace(\"\\\"\", \"'\") \n    \n    def add_asc(s):\n        pattern = re.compile(r'order by (?:\\w+ \\( \\S+ \\)|\\w+\\.\\w+|\\w+)(?: (?:\\+|\\-|\\<|\\<\\=|\\>|\\>\\=) (?:\\w+ \\( \\S+ \\)|\\w+\\.\\w+|\\w+))*')\n        if \"order by\" in s and \"asc\" not in s and \"desc\" not in s:\n            for p_str in pattern.findall(s):\n                s = s.replace(p_str, p_str + \" asc\")\n\n        return s\n\n    def remove_table_alias(s):\n        tables_aliases = Parser(s).tables_aliases\n        new_tables_aliases = {}\n        for i in range(1,11):\n            if \"t{}\".format(i) in tables_aliases.keys():\n                new_tables_aliases[\"t{}\".format(i)] = tables_aliases[\"t{}\".format(i)]\n        \n        tables_aliases = new_tables_aliases\n        for k, v in tables_aliases.items():\n            s = s.replace(\"as \" + k + \" \", \"\")\n            s = s.replace(k, v)\n        \n        return s\n    \n    processing_func = lambda x : (add_asc(lower(white_space_fix(double2single(remove_semicolon(x))))))\n\n    # processing_func = lambda x : remove_table_alias(add_asc(lower(white_space_fix(double2single(remove_semicolon(x))))))\n    \n    return processing_func(sql)\n\n# extract the skeleton of sql and natsql\ndef extract_skeleton(sql, db_schema):\n    table_names_original, table_dot_column_names_original, column_names_original = [], [], []\n    for table in db_schema[\"schema_items\"]:\n        table_name_original = table[\"table_name_original\"]\n        table_names_original.append(table_name_original)\n\n        for column_name_original in [\"*\"]+table[\"column_names_original\"]:\n            table_dot_column_names_original.append(table_name_original+\".\"+column_name_original)\n            column_names_original.append(column_name_original)\n    \n    parsed_sql = Parser(sql)\n    new_sql_tokens = []\n    for token in parsed_sql.tokens:\n        # mask table names\n        if token.value in table_names_original:\n            new_sql_tokens.append(\"_\")\n        # mask column names\n        elif token.value in column_names_original \\\n            or token.value in table_dot_column_names_original:\n            new_sql_tokens.append(\"_\")\n        # mask string values\n        elif token.value.startswith(\"'\") and token.value.endswith(\"'\"):\n            new_sql_tokens.append(\"_\")\n        # mask positive int number\n        elif token.value.isdigit():\n            new_sql_tokens.append(\"_\")\n        # mask negative int number\n        elif isNegativeInt(token.value):\n            new_sql_tokens.append(\"_\")\n        # mask float number\n        elif isFloat(token.value):\n            new_sql_tokens.append(\"_\")\n        else:\n            new_sql_tokens.append(token.value.strip())\n\n    sql_skeleton = \" \".join(new_sql_tokens)\n    \n    # remove JOIN ON keywords\n    sql_skeleton = sql_skeleton.replace(\"on _ = _ and _ = _\", \"on _ = _\")\n    sql_skeleton = sql_skeleton.replace(\"on _ = _ or _ = _\", \"on _ = _\")\n    sql_skeleton = sql_skeleton.replace(\" on _ = _\", \"\")\n    pattern3 = re.compile(\"_ (?:join _ ?)+\")\n    sql_skeleton = re.sub(pattern3, \"_ \", sql_skeleton)\n\n    # \"_ , _ , ..., _\" -> \"_\"\n    while(\"_ , _\" in sql_skeleton):\n        sql_skeleton = sql_skeleton.replace(\"_ , _\", \"_\")\n    \n    # remove clauses in WHERE keywords\n    ops = [\"=\", \"!=\", \">\", \">=\", \"<\", \"<=\"]\n    for op in ops:\n        if \"_ {} _\".format(op) in sql_skeleton:\n            sql_skeleton = sql_skeleton.replace(\"_ {} _\".format(op), \"_\")\n    while(\"where _ and _\" in sql_skeleton or \"where _ or _\" in sql_skeleton):\n        if \"where _ and _\"in sql_skeleton:\n            sql_skeleton = sql_skeleton.replace(\"where _ and _\", \"where _\")\n        if \"where _ or _\" in sql_skeleton:\n            sql_skeleton = sql_skeleton.replace(\"where _ or _\", \"where _\")\n\n    # remove additional spaces in the skeleton\n    while \"  \" in sql_skeleton:\n        sql_skeleton = sql_skeleton.replace(\"  \", \" \")\n\n    return sql_skeleton","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:57:17.143185Z","iopub.execute_input":"2025-05-10T04:57:17.143447Z","iopub.status.idle":"2025-05-10T04:57:17.168792Z","shell.execute_reply.started":"2025-05-10T04:57:17.143423Z","shell.execute_reply":"2025-05-10T04:57:17.168107Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"# sql_keywords = ['select', 'from', 'where', 'group', 'order', 'limit', 'intersect', 'union', \\\n#     'except', 'join', 'on', 'as', 'not', 'between', 'in', 'like', 'is', 'exists', 'max', 'min', \\\n#         'count', 'sum', 'avg', 'and', 'or', 'desc', 'asc']\n# import json\n# from tqdm import tqdm\n\n# def build_input_output_sequences(tables_path, spider_path, opt):\n#     with open(tables_path, 'r') as f:\n#         all_db_infos = json.load(f)\n\n#     with open(spider_path, 'r') as f:\n#         dataset = json.load(f)\n\n#     db_schemas = get_db_schemas(all_db_infos)\n#     input_output_pairs = []\n\n#     for data in tqdm(dataset):\n#         question = data[\"question\"].replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\").replace(\"\\u201c\", \"'\").replace(\"\\u201d\", \"'\").strip()\n#         db_id = data[\"db_id\"]\n#         sql = data[\"query\"].strip()\n\n#         input_sequence = question + \" | \"\n#         output_sequence = \"\"\n\n#         for table in db_schemas[db_id][\"schema_items\"]:\n#             db_contents = get_db_contents(\n#                 question,\n#                 table[\"table_name_original\"],\n#                 table[\"column_names_original\"],\n#                 db_id,\n#                 opt.db_path\n#             )\n\n#             # Add to input\n#             input_sequence += table[\"table_name_original\"] + \" : \"\n#             input_sequence += \" , \".join(table[\"column_names_original\"]) + \" | \"\n\n#             # Add to output\n#             output_sequence += table[\"table_name_original\"] + \" : \"\n#             output_sequence += \" , \".join(table[\"column_names_original\"]) + \" | \"\n\n#         # Add foreign keys to input\n#         # for fk in db_schemas[db_id][\"fk\"]:\n#         #     input_sequence += (\n#         #         fk[\"source_table_name_original\"] + \".\" + fk[\"source_column_name_original\"]\n#         #         + \" = \" +\n#         #         fk[\"target_table_name_original\"] + \".\" + fk[\"target_column_name_original\"]\n#         #         + \" | \"\n#         #     )\n\n#         input_output_pairs.append((input_sequence.strip(), output_sequence.strip()))\n\n#     return input_output_pairs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:57:17.169531Z","iopub.execute_input":"2025-05-10T04:57:17.169798Z","iopub.status.idle":"2025-05-10T04:57:17.184959Z","shell.execute_reply.started":"2025-05-10T04:57:17.169782Z","shell.execute_reply":"2025-05-10T04:57:17.184305Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"import json\nimport torch\nfrom datasets import Dataset\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom tqdm import tqdm\n# Setup\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load Data\nwith open(\"/kaggle/input/spider-data/train_spider.json\", \"r\") as f:\n    train_data = json.load(f)\n\nwith open(\"/kaggle/input/spider-data/tables.json\", \"r\") as f:\n    tables_data = json.load(f)\nclass Opt:\n    db_path = \"/kaggle/input/databased/database\"\n\nopt = Opt()\nimport json\nfrom tqdm import tqdm\n\ndef build_input_output_sequences(tables_path, spider_path, opt):\n    with open(tables_path, 'r') as f:\n        all_db_infos = json.load(f)\n\n    with open(spider_path, 'r') as f:\n        dataset = json.load(f)\n\n    db_schemas = get_db_schemas(all_db_infos)\n    input_output_pairs = []\n\n    for data in tqdm(dataset):\n        question = data[\"question\"].replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\").replace(\"\\u201c\", \"'\").replace(\"\\u201d\", \"'\").strip()\n        db_id = data[\"db_id\"]\n        sql = data[\"query\"].strip()\n\n        input_sequence = question + \" | \"\n        sql = data[\"sql\"]\n        db_id = data[\"db_id\"]\n        query = data[\"query\"]\n        table_entry = next(item for item in all_db_infos if item[\"db_id\"] == db_id)\n        column_names = table_entry[\"column_names_original\"]\n        table_names = table_entry[\"table_names_original\"]\n        aliases = {}\n    \n        used = extract_columns(sql, column_names, table_names, aliases)\n        output_sequence = format_used_columns(used).lower()\n\n        for table in db_schemas[db_id][\"schema_items\"]:\n            db_contents = get_db_contents(\n                question,\n                table[\"table_name_original\"],\n                table[\"column_names_original\"],\n                db_id,\n                opt.db_path\n            )\n\n            # Add to input\n            input_sequence += table[\"table_name_original\"] + \" : \"\n            input_sequence += \" , \".join(table[\"column_names_original\"]) + \" | \"\n\n        # Add foreign keys to input\n        for fk in db_schemas[db_id][\"fk\"]:\n            input_sequence += (\n                fk[\"source_table_name_original\"] + \".\" + fk[\"source_column_name_original\"]\n                + \" = \" +\n                fk[\"target_table_name_original\"] + \".\" + fk[\"target_column_name_original\"]\n                + \" | \"\n            )\n\n        input_output_pairs.append((input_sequence.strip(), output_sequence.strip()))\n# CAN ALSO KEEP PRIMARY KEY\n    return input_output_pairs\n\n\npairs = build_input_output_sequences(\"/kaggle/input/spider-data/tables.json\", \"/kaggle/input/spider-data/train_spider.json\", opt)\n\nfor inp, out in pairs[:3]:\n    print(\"INPUT:\", inp)\n    print(\"OUTPUT:\", out)\n    print(\"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:57:17.185761Z","iopub.execute_input":"2025-05-10T04:57:17.186029Z","iopub.status.idle":"2025-05-10T05:10:12.984770Z","shell.execute_reply.started":"2025-05-10T04:57:17.186004Z","shell.execute_reply":"2025-05-10T05:10:12.984033Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 7000/7000 [12:54<00:00,  9.04it/s]  ","output_type":"stream"},{"name":"stdout","text":"INPUT: How many heads of the departments are older than 56 ? | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting | management.head_id = head.head_id | management.department_id = department.department_id |\nOUTPUT: head: age , born_state , head_id , name\n================================================================================\nINPUT: List the name, born state and age of the heads of departments ordered by age. | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting | management.head_id = head.head_id | management.department_id = department.department_id |\nOUTPUT: head: age , born_state , name\n================================================================================\nINPUT: List the creation year, name and budget of each department. | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting | management.head_id = head.head_id | management.department_id = department.department_id |\nOUTPUT: department: budget_in_billions , creation , name\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"# input_texts = []\n# output_texts = []\n\n# for item in train_data:\n#     db_id = item['db_id']\n#     question = item['question']\n#     sql = item['query']\n#     db_schema = db_schemas[db_id]\n\n#     table_flags, columns = extract_tables_columns_flags(sql, db_schema)\n\n#     schema_str = \"\\n\".join(f\"- {t}: {', '.join(cols)}\" for t, cols in db_schema.items())\n\n#     input_text = f\"\"\"\n\n\n# Database Schema:\n# {schema_str}\n\n# Question:\n# {question}\n\n# Given this, Output \n# Tables : table_name -> True \n# Columns : table_name.column_name\n# \"\"\"\n\n#     table_str = \", \".join([f\"{t} -> {str(flag)}\" for t, flag in table_flags.items()])\n#     column_str = \", \".join(columns)\n#     output_text = f\"Tables: {table_str}\\nColumns: {column_str}\"\n\n#     input_texts.append(input_text)\n#     output_texts.append(output_text)\n\n#     # print(f\"OUTPUT : {output_text}\")\n\n# dataset = Dataset.from_dict({\n#     \"input_text\": input_texts,\n#     \"output_text\": output_texts\n# })\n\n\ninput_texts, output_texts = zip(*pairs)\n\n# Create HuggingFace Dataset\ndataset = Dataset.from_dict({\n    \"input_text\": list(input_texts),\n    \"output_text\": list(output_texts)\n})\n\n\ntokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-base\")\n\ndef tokenize_function(examples):\n    model_inputs = tokenizer(examples[\"input_text\"], max_length=512, padding=\"max_length\", truncation=True)\n    labels = tokenizer(examples[\"output_text\"], max_length=256, padding=\"max_length\", truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"input_text\", \"output_text\"])\n\ntrain_dataloader = DataLoader(\n    tokenized_dataset,\n    batch_size=4,\n    shuffle=True,\n    collate_fn=lambda x: {\n        key: torch.tensor([d[key] for d in x]) for key in x[0]\n    }\n)\n# Model\nmodel = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-base\")\npeft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.1)\nmodel = get_peft_model(model, peft_config)\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=5e-4)\n\n# Training\nmodel.train()\nfor epoch in range(5):\n    total_loss = 0\n    for batch in tqdm(train_dataloader,desc=\"train:\"):\n        optimizer.zero_grad()\n        batch = {k: v.to(device) for k, v in batch.items()}\n    \n        outputs = model(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            labels=batch[\"labels\"]\n        )\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    \n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1} - Average Training Loss: {avg_loss:.4f}\")\n\n# Save\nmodel.save_pretrained(\"./model_output\")\ntokenizer.save_pretrained(\"./model_output\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T05:10:12.985608Z","iopub.execute_input":"2025-05-10T05:10:12.985814Z","iopub.status.idle":"2025-05-10T05:59:24.386392Z","shell.execute_reply.started":"2025-05-10T05:10:12.985797Z","shell.execute_reply":"2025-05-10T05:59:24.385820Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"089d6e9fb682440bb88b759ab5fa5c7b"}},"metadata":{}},{"name":"stderr","text":"train:: 100%|██████████| 1750/1750 [09:48<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average Training Loss: 0.1649\n","output_type":"stream"},{"name":"stderr","text":"train:: 100%|██████████| 1750/1750 [09:48<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Average Training Loss: 0.0557\n","output_type":"stream"},{"name":"stderr","text":"train:: 100%|██████████| 1750/1750 [09:48<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Average Training Loss: 0.0436\n","output_type":"stream"},{"name":"stderr","text":"train:: 100%|██████████| 1750/1750 [09:47<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Average Training Loss: 0.0350\n","output_type":"stream"},{"name":"stderr","text":"train:: 100%|██████████| 1750/1750 [09:46<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Average Training Loss: 0.0286\n","output_type":"stream"},{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"('./model_output/tokenizer_config.json',\n './model_output/special_tokens_map.json',\n './model_output/spiece.model',\n './model_output/added_tokens.json')"},"metadata":{}}],"execution_count":78},{"cell_type":"code","source":"from collections import Counter\nimport re\ndef parse_schema_output(text):\n    \"\"\"Robustly parse 'table : col1, col2 | table2:col1,col2' into dict.\"\"\"\n    schema = defaultdict(set)\n    parts = text.strip().split(\"|\")\n\n    for part in parts:\n        if \":\" not in part:\n            continue  # skip invalid parts\n        table, cols = map(str.strip, part.split(\":\", 1))\n        col_list = re.split(r\"\\s*,\\s*\", cols.strip())\n        for col in col_list:\n            if col:\n                schema[table].add(col)\n    return schema\nfrom tqdm import tqdm\n\ndef evaluate_predictions(pairs, predictions):\n    assert len(pairs) == len(predictions), \"Mismatch between predictions and ground truth pairs\"\n\n    table_correct = 0\n    table_total = 0\n    table_recall_total = 0\n\n    column_correct = 0\n    column_total = 0\n    column_recall_total = 0\n\n    print(\"\\nSample Predictions (First 10):\\n\" + \"-\"*50)\n\n    for i, ((_, ground_truth), prediction) in enumerate(tqdm(zip(pairs, predictions), total=len(pairs))):\n        gt_schema = parse_schema_output(ground_truth)\n        pred_schema = parse_schema_output(prediction)\n\n        if i < 10:\n            print(f\"\\nExample {i+1}\")\n            print(\"Ground Truth:\")\n            print(ground_truth)\n            print(\"Prediction:\")\n            print(prediction)\n            print(\"-\" * 50)\n\n        # Evaluate tables\n        gt_tables = set(gt_schema.keys())\n        pred_tables = set(pred_schema.keys())\n\n        table_correct += len(gt_tables & pred_tables)\n        table_total += len(pred_tables)\n        table_recall_total += len(gt_tables)\n\n        # Evaluate columns\n        for table in gt_tables:\n            gt_cols = gt_schema.get(table, set())\n            pred_cols = pred_schema.get(table, set())\n\n            column_correct += len(gt_cols & pred_cols)\n            column_total += len(pred_cols)\n            column_recall_total += len(gt_cols)\n\n    table_accuracy = table_correct / table_total if table_total else 0.0\n    table_recall = table_correct / table_recall_total if table_recall_total else 0.0\n\n    column_accuracy = column_correct / column_total if column_total else 0.0\n    column_recall = column_correct / column_recall_total if column_recall_total else 0.0\n\n    print(\"\\nFinal Evaluation Metrics\\n\" + \"=\"*50)\n    print(f\"Table Accuracy : {table_accuracy:.4f}\")\n    print(f\"Table Recall   : {table_recall:.4f}\")\n    print(f\"Column Accuracy: {column_accuracy:.4f}\")\n    print(f\"Column Recall  : {column_recall:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T05:59:24.387145Z","iopub.execute_input":"2025-05-10T05:59:24.387401Z","iopub.status.idle":"2025-05-10T05:59:24.396467Z","shell.execute_reply.started":"2025-05-10T05:59:24.387374Z","shell.execute_reply":"2025-05-10T05:59:24.395819Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"pairs2 = build_input_output_sequences(\n    \"/kaggle/input/spider-data/tables.json\",\n    \"/kaggle/input/spider-data/dev.json\",\n    opt\n)\n\ninputs = [p[0] for p in pairs2]\ngold_outputs = [p[1] for p in pairs2]\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\n# model = T5ForConditionalGeneration.from_pretrained(\"path/to/your/model\")\n# tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")  # or your custom tokenizer\n\nmodel = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.eval()\n\nfrom tqdm import tqdm\nimport torch\nimport random\n\nbatch_size = 8  # Try increasing this if you have enough VRAM\nnum_beams = 6\nnum_return_sequences = 4\n\nprint_indices = set(random.sample(range(len(inputs)), 2))\npredictions = []\n\nprint(\"\\nGenerating predictions with beam search (batched)...\\n\")\n\nfor start_idx in tqdm(range(0, len(inputs), batch_size)):\n    end_idx = min(start_idx + batch_size, len(inputs))\n    batch_inputs = inputs[start_idx:end_idx]\n\n    encoded = tokenizer(batch_inputs, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **encoded,\n            max_length=256,\n            num_beams=num_beams,\n            length_penalty = 0.8,\n            num_return_sequences=num_return_sequences,\n            early_stopping=True\n        )\n\n    decoded = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n\n    # Group decoded outputs (num_return_sequences per input)\n    grouped = [decoded[i:i + num_return_sequences] for i in range(0, len(decoded), num_return_sequences)]\n\n    for i, group in enumerate(grouped):\n        predictions.append(group[0])  # main prediction\n\n        global_idx = start_idx + i\n        if global_idx in print_indices:\n            print(f\"\\nBeam Search Results for Example {global_idx + 1}\")\n            print(\"-\" * 50)\n            print(\"Input:\")\n            print(inputs[global_idx].strip())\n            print(\"\\nBeam Outputs:\")\n            for j, out in enumerate(group):\n                print(f\"[Beam {j+1}]: {out}\")\n            print(\"-\" * 50)\n\n# Evaluation\n\nprint(predictions[:5])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T05:59:24.398595Z","iopub.execute_input":"2025-05-10T05:59:24.398779Z","iopub.status.idle":"2025-05-10T06:04:54.206659Z","shell.execute_reply.started":"2025-05-10T05:59:24.398765Z","shell.execute_reply":"2025-05-10T06:04:54.206028Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1034/1034 [00:58<00:00, 17.55it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nGenerating predictions with beam search (batched)...\n\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/130 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n 25%|██▍       | 32/130 [00:33<01:44,  1.06s/it]","output_type":"stream"},{"name":"stdout","text":"\nBeam Search Results for Example 250\n--------------------------------------------------\nInput:\nWhat are flight numbers of flights arriving at Airport \"APG\"? | airlines : uid , airline , abbreviation , country | airports : city , airportcode , airportname , country , countryabbrev | flights : airline , flightno , sourceairport , destairport | flights.destairport = airports.airportcode | flights.sourceairport = airports.airportcode |\n\nBeam Outputs:\n[Beam 1]: airports: airportcode , airportname | flights: airline , flightno\n[Beam 2]: flights: airline , flightno , sourceairport | airports: airportcode , airportname\n[Beam 3]: airports: airportcode , city | flights: airline , flightno\n[Beam 4]: airports: airportcode , city | flights: airline , flightno , sourceairport\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 75/130 [03:06<02:38,  2.88s/it]","output_type":"stream"},{"name":"stdout","text":"\nBeam Search Results for Example 600\n--------------------------------------------------\nInput:\nWhat is the content of TV Channel with serial name \"Sky Radio\"? | tv_channel : id , series_name , country , language , content , pixel_aspect_ratio_par , hight_definition_tv , pay_per_view_ppv , package_option | tv_series : id , episode , air_date , rating , share , 18_49_rating_share , viewers_m , weekly_rank , channel | cartoon : id , title , directed_by , written_by , original_air_date , production_code , channel | tv_series.channel = tv_channel.id | cartoon.channel = tv_channel.id |\n\nBeam Outputs:\n[Beam 1]: tv_channel: content , series_name\n[Beam 2]: tv_channel: content , id , series_name\n[Beam 3]: tv_channel: content , serial_name\n[Beam 4]: tv_channel: content , id , serial_name\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 130/130 [04:30<00:00,  2.08s/it]","output_type":"stream"},{"name":"stdout","text":"['singer: age , country , song_name , song_release_year , is_male , singer_id', 'singer: age , country , song_name , song_release_year , is_male , name', 'singer: age , country', 'singer: age , country', 'singer: age , country']\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"evaluate_predictions(pairs2, predictions)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:04:54.207415Z","iopub.execute_input":"2025-05-10T06:04:54.207624Z","iopub.status.idle":"2025-05-10T06:04:54.228667Z","shell.execute_reply.started":"2025-05-10T06:04:54.207608Z","shell.execute_reply":"2025-05-10T06:04:54.227729Z"}},"outputs":[{"name":"stdout","text":"\nSample Predictions (First 10):\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1034/1034 [00:00<00:00, 69055.78it/s]","output_type":"stream"},{"name":"stdout","text":"\nExample 1\nGround Truth:\nsinger: age , country , is_male , name , singer_id , song_name , song_release_year\nPrediction:\nsinger: age , country , song_name , song_release_year , is_male , singer_id\n--------------------------------------------------\n\nExample 2\nGround Truth:\nsinger: age , country , is_male , name , singer_id , song_name , song_release_year\nPrediction:\nsinger: age , country , song_name , song_release_year , is_male , name\n--------------------------------------------------\n\nExample 3\nGround Truth:\nsinger: age , country , name\nPrediction:\nsinger: age , country\n--------------------------------------------------\n\nExample 4\nGround Truth:\nsinger: age , country , name\nPrediction:\nsinger: age , country\n--------------------------------------------------\n\nExample 5\nGround Truth:\nsinger: age , country\nPrediction:\nsinger: age , country\n--------------------------------------------------\n\nExample 6\nGround Truth:\nsinger: age , country\nPrediction:\nsinger: age\n--------------------------------------------------\n\nExample 7\nGround Truth:\nsinger: age , song_name , song_release_year\nPrediction:\nsinger: age , song_name , song_release_year\n--------------------------------------------------\n\nExample 8\nGround Truth:\nsinger: age , song_name , song_release_year\nPrediction:\nsinger: age , song_name , song_release_year\n--------------------------------------------------\n\nExample 9\nGround Truth:\nsinger: age , country\nPrediction:\nsinger: country , name\n--------------------------------------------------\n\nExample 10\nGround Truth:\nsinger: age , country\nPrediction:\nsinger: age , country\n--------------------------------------------------\n\nFinal Evaluation Metrics\n==================================================\nTable Accuracy : 0.9475\nTable Recall   : 0.7626\nColumn Accuracy: 0.8924\nColumn Recall  : 0.6161\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"!pip install sql_metadata\nimport re\nfrom sql_metadata import Parser\ndef isNegativeInt(string):\n    if string.startswith(\"-\") and string[1:].isdigit():\n        return True\n    else:\n        return False\n\ndef isFloat(string):\n    if string.startswith(\"-\"):\n        string = string[1:]\n    \n    s = string.split(\".\")\n    if len(s)>2:\n        return False\n    else:\n        for s_i in s:\n            if not s_i.isdigit():\n                return False\n        return True\ndef build_input_output_sequences(tables_path, spider_path, opt):\n    with open(tables_path, 'r') as f:\n        all_db_infos = json.load(f)\n\n    with open(spider_path, 'r') as f:\n        dataset = json.load(f)\n\n    db_schemas = get_db_schemas(all_db_infos)\n    input_output_pairs = []\n\n    for data in tqdm(dataset):\n        question = data[\"question\"].replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\").replace(\"\\u201c\", \"'\").replace(\"\\u201d\", \"'\").strip()\n        db_id = data[\"db_id\"]\n        sql = data[\"query\"].strip()\n\n        input_sequence = question + \" | \"\n        sql = data[\"sql\"]\n        db_id = data[\"db_id\"]\n        query = data[\"query\"]\n        table_entry = next(item for item in all_db_infos if item[\"db_id\"] == db_id)\n        column_names = table_entry[\"column_names_original\"]\n        table_names = table_entry[\"table_names_original\"]\n        table_names.append(\"t1\")\n        table_names.append(\"t2\")\n        table_names.append(\"T1\")\n        table_names.append(\"T2\")\n        aliases = {}\n    \n        used = extract_columns(sql, column_names, table_names, aliases)\n        schema = format_used_columns(used).lower()\n\n        input_sequence += schema\n        input_sequence +=' | '\n\n        # Add foreign keys to input\n        # for fk in db_schemas[db_id][\"fk\"]:\n        #     input_sequence += (\n        #         fk[\"source_table_name_original\"] + \".\" + fk[\"source_column_name_original\"]\n        #         + \" = \" +\n        #         fk[\"target_table_name_original\"] + \".\" + fk[\"target_column_name_original\"]\n        #         + \" | \"\n        #     )\n         \n        norm_sql = normalization(query).strip()\n        sql_skeleton = extract_skeleton(norm_sql, db_schemas[db_id]).strip()\n        output_sequence = f\"{sql_skeleton} | {norm_sql}\"\n  \n        \n\n        input_output_pairs.append((input_sequence.strip(), output_sequence.strip()))\n# CAN ALSO KEEP PRIMARY KEY\n    return input_output_pairs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:04:54.229686Z","iopub.execute_input":"2025-05-10T06:04:54.230025Z","iopub.status.idle":"2025-05-10T06:05:13.393422Z","shell.execute_reply.started":"2025-05-10T06:04:54.229969Z","shell.execute_reply":"2025-05-10T06:05:13.392563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pairs_train = build_input_output_sequences(\"/kaggle/input/spider-data/tables.json\", \"/kaggle/input/spider-data/train_spider.json\", opt)\n\nfor inp, out in pairs_train[:3]:\n    print(\"INPUT:\", inp)\n    print(\"OUTPUT:\", out)\n    print(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:04:54.229686Z","iopub.execute_input":"2025-05-10T06:04:54.230025Z","iopub.status.idle":"2025-05-10T06:05:13.393422Z","shell.execute_reply.started":"2025-05-10T06:04:54.229969Z","shell.execute_reply":"2025-05-10T06:05:13.392563Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sql_metadata in /usr/local/lib/python3.11/dist-packages (2.17.0)\nRequirement already satisfied: sqlparse<0.6.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from sql_metadata) (0.5.3)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7000/7000 [00:16<00:00, 436.12it/s]","output_type":"stream"},{"name":"stdout","text":"INPUT: How many heads of the departments are older than 56 ? | head: age , born_state , head_id , name |\nOUTPUT: select count ( _ ) from _ where _ | select count ( * ) from head where age > 56\n================================================================================\nINPUT: List the name, born state and age of the heads of departments ordered by age. | head: age , born_state , name |\nOUTPUT: select _ from _ order by _ asc | select name , born_state , age from head order by age asc\n================================================================================\nINPUT: List the creation year, name and budget of each department. | department: budget_in_billions , creation , name |\nOUTPUT: select _ from _ | select creation , name , budget_in_billions from department\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# input_texts = []\n# output_texts = []\n\n# for item in train_data:\n#     db_id = item['db_id']\n#     question = item['question']\n#     sql = item['query']\n#     db_schema = db_schemas[db_id]\n\n#     table_flags, columns = extract_tables_columns_flags(sql, db_schema)\n\n#     schema_str = \"\\n\".join(f\"- {t}: {', '.join(cols)}\" for t, cols in db_schema.items())\n\n#     input_text = f\"\"\"\n\n\n# Database Schema:\n# {schema_str}\n\n# Question:\n# {question}\n\n# Given this, Output \n# Tables : table_name -> True \n# Columns : table_name.column_name\n# \"\"\"\n\n#     table_str = \", \".join([f\"{t} -> {str(flag)}\" for t, flag in table_flags.items()])\n#     column_str = \", \".join(columns)\n#     output_text = f\"Tables: {table_str}\\nColumns: {column_str}\"\n\n#     input_texts.append(input_text)\n#     output_texts.append(output_text)\n\n#     # print(f\"OUTPUT : {output_text}\")\n\n# dataset = Dataset.from_dict({\n#     \"input_text\": input_texts,\n#     \"output_text\": output_texts\n# })\n\n\ninput_texts, output_texts = zip(*pairs_train)\n\n# Create HuggingFace Dataset\ndataset = Dataset.from_dict({\n    \"input_text\": list(input_texts),\n    \"output_text\": list(output_texts)\n})\n\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n\ndef tokenize_function(examples):\n    model_inputs = tokenizer(examples[\"input_text\"], max_length=512, padding=\"max_length\", truncation=True)\n    labels = tokenizer(examples[\"output_text\"], max_length=256, padding=\"max_length\", truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"input_text\", \"output_text\"])\n\ntrain_dataloader = DataLoader(\n    tokenized_dataset,\n    batch_size=4,\n    shuffle=True,\n    collate_fn=lambda x: {\n        key: torch.tensor([d[key] for d in x]) for key in x[0]\n    }\n)\n# Model\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\npeft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.1)\nmodel = get_peft_model(model, peft_config)\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=5e-4)\n\n# Training\nmodel.train()\nfor epoch in range(3):\n    total_loss = 0\n    for batch in tqdm(train_dataloader,desc=\"train:\"):\n        optimizer.zero_grad()\n        batch = {k: v.to(device) for k, v in batch.items()}\n    \n        outputs = model(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            labels=batch[\"labels\"]\n        )\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    \n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1} - Average Training Loss: {avg_loss:.4f}\")\n\n# Save\nmodel.save_pretrained(\"./encdec_output\")\ntokenizer.save_pretrained(\"./encdec_output\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:07:40.465293Z","iopub.execute_input":"2025-05-10T06:07:40.465561Z","iopub.status.idle":"2025-05-10T06:39:47.253454Z","shell.execute_reply.started":"2025-05-10T06:07:40.465543Z","shell.execute_reply":"2025-05-10T06:39:47.252826Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b4beeefabdb403380dff543ae983af3"}},"metadata":{}},{"name":"stderr","text":"train:: 100%|██████████| 1750/1750 [10:40<00:00,  2.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average Training Loss: 1.1132\n","output_type":"stream"},{"name":"stderr","text":"train:: 100%|██████████| 1750/1750 [10:39<00:00,  2.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Average Training Loss: 0.0918\n","output_type":"stream"},{"name":"stderr","text":"train:: 100%|██████████| 1750/1750 [10:40<00:00,  2.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Average Training Loss: 0.0708\n","output_type":"stream"},{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"('./encdec_output/tokenizer_config.json',\n './encdec_output/special_tokens_map.json',\n './encdec_output/spiece.model',\n './encdec_output/added_tokens.json')"},"metadata":{}}],"execution_count":84},{"cell_type":"code","source":"pairs2_encdec = build_input_output_sequences(\n    \"/kaggle/input/spider-data/tables.json\",\n    \"/kaggle/input/spider-data/dev.json\",\n    opt\n)\n\ninputs = [p[0] for p in pairs2_encdec]\ngold_outputs = [p[1] for p in pairs2_encdec]\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\n# model = T5ForConditionalGeneration.from_pretrained(\"path/to/your/model\")\n# tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")  # or your custom tokenizer\n\nmodel = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.eval()\n\nfrom tqdm import tqdm\nimport torch\nimport random\n\nbatch_size = 8  # Try increasing this if you have enough VRAM\nnum_beams = 4\nnum_return_sequences = 4\n\nprint_indices = set(random.sample(range(len(inputs)), 2))\npredictions = []\n\nprint(\"\\nGenerating predictions with beam search (batched)...\\n\")\n\nfor start_idx in tqdm(range(0, len(inputs), batch_size)):\n    end_idx = min(start_idx + batch_size, len(inputs))\n    batch_inputs = inputs[start_idx:end_idx]\n\n    encoded = tokenizer(batch_inputs, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **encoded,\n            max_length=256,\n            num_beams=num_beams,\n            length_penalty = 0.8,\n            num_return_sequences=num_return_sequences,\n            early_stopping=True\n        )\n\n    decoded = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n\n    # Group decoded outputs (num_return_sequences per input)\n    grouped = [decoded[i:i + num_return_sequences] for i in range(0, len(decoded), num_return_sequences)]\n\n    for i, group in enumerate(grouped):\n        predictions.append(group[0])  # main prediction\n\n        global_idx = start_idx + i\n        if global_idx in print_indices:\n            print(f\"\\nBeam Search Results for Example {global_idx + 1}\")\n            print(\"-\" * 50)\n            print(\"Input:\")\n            print(inputs[global_idx].strip())\n            print(\"\\nBeam Outputs:\")\n            for j, out in enumerate(group):\n                print(f\"[Beam {j+1}]: {out}\")\n            print(\"-\" * 50)\n\n# Evaluation\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:07:33.040740Z","iopub.status.idle":"2025-05-10T06:07:33.041064Z","shell.execute_reply.started":"2025-05-10T06:07:33.040869Z","shell.execute_reply":"2025-05-10T06:07:33.040882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(predictions[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:07:33.042390Z","iopub.status.idle":"2025-05-10T06:07:33.043100Z","shell.execute_reply.started":"2025-05-10T06:07:33.042930Z","shell.execute_reply":"2025-05-10T06:07:33.042946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"norm_sql_list = [pred.split(\"|\", 1)[1].strip() for pred in predictions]\n\nprint(norm_sql_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:07:33.043885Z","iopub.status.idle":"2025-05-10T06:07:33.044185Z","shell.execute_reply.started":"2025-05-10T06:07:33.044055Z","shell.execute_reply":"2025-05-10T06:07:33.044068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/taoyds/spider.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:07:33.045667Z","iopub.status.idle":"2025-05-10T06:07:33.045980Z","shell.execute_reply.started":"2025-05-10T06:07:33.045819Z","shell.execute_reply":"2025-05-10T06:07:33.045833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"/kaggle/input/spider-data/dev.json\", \"r\") as f:\n    val_data = json.load(f)\ndb_ids = []\nlabels=[]\n\nfor item in val_data:\n    db_id = item['db_id']\n    sql = item['query']\n    labels.append(sql)\n    db_ids.append(db_id)\nprediction_entries = []\nfor pred, db_id in zip(norm_sql_list, db_ids):\n    prediction_entries.append({\n        \"query\": pred,\n        # \"question\": gold,  # Not used by evaluator\n        \"db_id\": db_id\n    })\n\nwith open(\"full.json\", \"w\") as f:\n    json.dump(prediction_entries, f, indent=2)    \nfor i in range(30,50):\n    print(\"=\"*50)\n    print(f\"Input:\\n{input_texts[i]}\")\n    print(f\"Target:\\n{labels[i]}\")\n    print(f\"Prediction:\\n{norm_sql_list[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:07:33.046777Z","iopub.status.idle":"2025-05-10T06:07:33.047153Z","shell.execute_reply.started":"2025-05-10T06:07:33.046925Z","shell.execute_reply":"2025-05-10T06:07:33.046939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# Load gold data from Spider dev.json\nwith open(\"/kaggle/input/spider-data/dev.json\") as f:\n    gold_data = json.load(f)\n\n# Write gold.sql (each line: <GOLD_SQL> \\t <DB_ID>)\nwith open(\"gold.sql\", \"w\") as f:\n    for example in gold_data:\n        f.write(f\"{example['query']}\\t{example['db_id']}\\n\")\n\n# Write pred.sql (each line: predicted SQL)\n# `preds` must be in the same order as `gold_data`\nwith open(\"pred.sql\", \"w\") as f:\n    for pred in norm_sql_list:\n        f.write(pred.strip() + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:07:33.048727Z","iopub.status.idle":"2025-05-10T06:07:33.049051Z","shell.execute_reply.started":"2025-05-10T06:07:33.048866Z","shell.execute_reply":"2025-05-10T06:07:33.048878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python3 /kaggle/working/spider/evaluation.py \\\n    --gold gold.sql \\\n    --pred pred.sql \\\n    --etype all \\\n    --db /kaggle/input/databased/database \\\n    --table /kaggle/input/spider-data/tables.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:07:33.050054Z","iopub.status.idle":"2025-05-10T06:07:33.050260Z","shell.execute_reply.started":"2025-05-10T06:07:33.050161Z","shell.execute_reply":"2025-05-10T06:07:33.050170Z"}},"outputs":[],"execution_count":null}]}